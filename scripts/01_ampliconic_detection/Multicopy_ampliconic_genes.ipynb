{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Detection of Ampliconic gene families</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BCBio import GFF\n",
    "from BCBio.GFF import GFFExaminer\n",
    "examiner = GFFExaminer()\n",
    "\n",
    "from Bio import SeqIO\n",
    "from pprint import pprint\n",
    "import copy \n",
    "import gspread\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data_dir` folder into which reference files should be loaded prior to analysis,\n",
    "expected structure\n",
    "\n",
    "```bash\n",
    "{data_dir}\n",
    "├── references\n",
    "│   ├── GorGor\n",
    "│   │   └── ...  # https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_029281585.2/\n",
    "│   ├── HomSap\n",
    "│   │   └── ...  # https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_009914755.1/\n",
    "│   ├── PanPan\n",
    "│   │   └── ...  # https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_029289425.2/\n",
    "│   ├── PanTro\n",
    "│   │   └── ...  # https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_028858775.2/\n",
    "│   ├── PonAbe\n",
    "│   │   └── ...  # https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_028885655.2/\n",
    "│   ├── PonPyg\n",
    "│   │   └── ...  # https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_028885625.2/\n",
    "│   └── SymSyn\n",
    "│       └── ...  # https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_028878055.3/\n",
    "│   └── MacFas\n",
    "│       └── ...  # https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_037993035.2/\n",
    "├── work_dir\n",
    "│       └── ...\n",
    "└── AdditionalFile2-SeqClasses.tsv # Download sequence classes file from additional data files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory for the data\n",
    "data_dir = \"/home/emma/Amplicons/Workspaces/emma/downloaded_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = f\"{data_dir}/work_dir/x_multicopy\" #Creates a new working directory if it doesn't exist already\n",
    "os.makedirs(work_dir, exist_ok=True) # ensures no error is raised if the directory already exists\n",
    "\n",
    "#Define the list of dictionaries (data) containing genome information for different species\n",
    "data = [\n",
    "    {'species':'PanTro',\n",
    "     'data': {'chr_y': \"NC_072422.2\",\n",
    "              'chr_x': \"NC_072421.2\",\n",
    "              'path_to_annotation_NCBI': f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/genomic.gff\", #annotation file \n",
    "              'path_to_annotation_NCBI_chry': f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/genomic_chrY.gff\", #Y specific annotation\n",
    "              'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/GCF_028858775.2.gff3\", #path to an alternate annotation format (generated by CAT pipeline)\n",
    "              'ref':  f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/GCF_028858775.2_NHGRI_mPanTro3-v2.0_pri_genomic.fna\", # primary reference genome file\n",
    "              'rna':  f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/rna.fna\", #RNA sequences \n",
    "              'prot': f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/protein.faa\", #protein sequences (FASTA format)\n",
    "              'cds': f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/cds_from_genomic.fna\", #coding DNA seq derived from genome\n",
    " }},\n",
    "    {'species':'HomSap',\n",
    "     'data': {'chr_y': \"NC_060948.1\",\n",
    "              'chr_x': \"NC_060947.1\",\n",
    "              'path_to_annotation_NCBI': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/genomic.gff\",\n",
    "              'path_to_annotation_NCBI_chry': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/genomic_chrY.gff\",\n",
    "              'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/hg38.gff3\",\n",
    "              'ref': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/GCF_009914755.1_T2T-CHM13v2.0_genomic.fna\",\n",
    "              'cds': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/cds_from_genomic.fna\",\n",
    "              'prot': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/protein.faa\",\n",
    "              }},\n",
    "     {'species':'PanPan',\n",
    "     'data': {'chr_y': \"NC_073273.2\",\n",
    "              'chr_x': \"NC_073272.2\",\n",
    "              'path_to_annotation_NCBI': f\"{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/genomic.gff\",\n",
    "              'path_to_annotation_NCBI_chry': f\"{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/genomic_chrY.gff\",\n",
    "              'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/GCF_029289425.2.gff3\",\n",
    "              'ref': f\"{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/GCF_029289425.2_NHGRI_mPanPan1-v2.0_pri_genomic.fna\",\n",
    "              'cds': f'{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/cds_from_genomic.fna',\n",
    "              'prot': f'{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/protein.faa',\n",
    "              }},\n",
    "      {'species':'GorGor',\n",
    "     'data': {'chr_y': \"NC_073248.2\",\n",
    "              'chr_x': \"NC_073247.2\",\n",
    "              'path_to_annotation_NCBI': f\"{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/genomic.gff\",\n",
    "              'path_to_annotation_NCBI_chry': f\"{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/genomic_chrY.gff\",\n",
    "              'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/GCF_029281585.2.gff3\",\n",
    "              'ref': f'{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/GCF_029281585.2_NHGRI_mGorGor1-v2.0_pri_genomic.fna',\n",
    "              'cds': f'{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/cds_from_genomic.fna',\n",
    "              'prot': f'{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/protein.faa',\n",
    "              }},\n",
    "    {'species':'PonPyg',\n",
    "     'data': {'chr_y': \"NC_072397.2\",\n",
    "              'chr_x': \"NC_072396.2\",\n",
    "              'path_to_annotation_NCBI': f\"{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/genomic.gff\",\n",
    "              'path_to_annotation_NCBI_chry': f\"{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/genomic_chrY.gff\",\n",
    "              'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/GCF_028885625.2.gff3\",\n",
    "              'ref': f'{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/GCF_028885625.2_NHGRI_mPonPyg2-v2.0_pri_genomic.fna',\n",
    "              'cds': f'{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/cds_from_genomic.fna',\n",
    "              'prot': f'{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/protein.faa',\n",
    "              }},\n",
    "    {'species':'PonAbe',\n",
    "     'data': {'chr_y': \"NC_072009.2\",\n",
    "              'chr_x': \"NC_072008.2\",\n",
    "              'path_to_annotation_NCBI': f\"{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/genomic.gff\",\n",
    "              'path_to_annotation_NCBI_chry': f\"{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/genomic_chrY.gff\",\n",
    "              'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/GCF_028885655.2.gff3\",\n",
    "              'ref': f'{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/GCF_028885655.2_NHGRI_mPonAbe1-v2.0_pri_genomic.fna',\n",
    "              'cds': f'{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/cds_from_genomic.fna',\n",
    "              'prot': f'{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/protein.faa',\n",
    "              }},    \n",
    "    {'species':'SymSyn',\n",
    "      'data': {'chr_y': \"NC_072448.2\",\n",
    "               'chr_x': \"NC_072447.2\",\n",
    "               'ref': f'{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/GCF_028878055.3_NHGRI_mSymSyn1-v2.1_pri_genomic.fna',\n",
    "               'path_to_annotation_NCBI': f\"{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/genomic.gff\",\n",
    "               'path_to_annotation_NCBI_chry': f\"{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/genomic_chrY.gff\",\n",
    "               'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/GCF_028878055.3.gff3\",\n",
    "               'cds': f'{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/cds_from_genomic.fna',\n",
    "               'prot': f'{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/protein.faa',\n",
    "               }},\n",
    "    {'species':'MacFas',\n",
    "      'data': {'chr_y': \"NC_132903.1\",\n",
    "               'chr_x': \"NC_088395.1\",\n",
    "               'ref': f'{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/GCF_037993035.2_T2T-MFA8v1.1_genomic.fna',\n",
    "               'path_to_annotation_NCBI': f\"{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/genomic.gff\",\n",
    "               'cds': f'{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/cds_from_genomic.fna',\n",
    "               'prot': f'{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/protein.faa',\n",
    "               }},\n",
    "]\n",
    "\n",
    "# Maps species identifiers to their common name\n",
    "species_to_sequence_spec = {\n",
    "    'PanTro': 'chimpanzee',\n",
    "    'HomSap': 'human',\n",
    "    'PanPan': 'bonobo',\n",
    "    'GorGor': 'gorilla',\n",
    "    'PonPyg': 'b-orang',\n",
    "    'PonAbe': 's-orang',\n",
    "    'SymSyn': 'siamang',\n",
    "    'MacFas': 'macaque'\n",
    "}\n",
    "\n",
    "# Extracting species names -> list of species identifiers by iterating t\n",
    "species_list = [d['species'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset annotation file to only include chromosome X & Y\n",
    "for species in species_list:\n",
    "\n",
    "    annotation_file =  [d for d in data if d['species'] == species][0]['data']['path_to_annotation_NCBI']\n",
    "    annotation_file_x = annotation_file.replace(\".gff\", \"_chrX.gff\")\n",
    "    chr = [d for d in data if d['species'] == species][0]['data']['chr_x']\n",
    "    print(species, annotation_file, chr)\n",
    "\n",
    "    ! cat $annotation_file | grep -v \"#\" | grep -w $chr | sort -k1,1V -k4,4n -k5,5rn -k3,3r - > $annotation_file_x\n",
    "\n",
    "    annotation_file_y = annotation_file.replace(\".gff\", \"_chrY.gff\")\n",
    "    chr = [d for d in data if d['species'] == species][0]['data']['chr_y']\n",
    "    print(species, annotation_file, chr)\n",
    "\n",
    "    ! cat $annotation_file | grep -v \"#\" | grep -w $chr | sort -k1,1V -k4,4n -k5,5rn -k3,3r - > $annotation_file_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chromosome X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs around 20s without extracting the sequence\n",
    "#runs around 1m with extracting the sequence\n",
    "#only collect ID's will extract sequence from cds_from_genomic.fna\n",
    "record_dict = {}\n",
    "\n",
    "for species in species_list:\n",
    "\n",
    "\n",
    "    annotation_file =  [d for d in data if d['species'] == species][0]['data']['path_to_annotation_NCBI'].replace(\".gff\", \"_chrX.gff\")\n",
    "    handle = open(annotation_file)\n",
    "\n",
    "    # pprint.pprint(examiner.parent_child_map(handle))\n",
    "\n",
    "    # assert False\n",
    "\n",
    "    # records = GFF.parse(handle,base_dict=seq_dict)\n",
    "    records = GFF.parse(handle)\n",
    "    for rec in records:\n",
    "        protss = set()\n",
    "        for feature in rec.features:\n",
    "            if feature.type != \"gene\":\n",
    "                continue\n",
    "            if \"description\" in feature.qualifiers:\n",
    "                description = feature.qualifiers[\"description\"][0]\n",
    "\n",
    "                gene_id = feature.id.replace(\"gene-\", \"\")\n",
    "                gene_spec_id = gene_id + \"_\" + species\n",
    "                    \n",
    "                # print(feature.qualifiers)\n",
    "                for subf in feature.sub_features:\n",
    "\n",
    "                    if subf.type == \"mRNA\":\n",
    "                        mrna_id = subf.id.replace(\"rna-\", \"\")\n",
    "                        for subsubf in subf.sub_features:\n",
    "                            if subsubf.type != \"CDS\":\n",
    "                                continue\n",
    "                            # cds_id = subsubf.id.replace(\"cds-\", \"\")\n",
    "                            prot_id = subsubf.qualifiers[\"protein_id\"][0]\n",
    "                            if prot_id in protss:\n",
    "                                continue\n",
    "\n",
    "                            protss.add(prot_id)\n",
    "                            element = {}\n",
    "                            element[\"specie\"] = species\n",
    "                            element[\"name\"] = description\n",
    "                            element[\"id\"] = prot_id\n",
    "                            element[\"mrna_id\"] = mrna_id\n",
    "                            element[\"gene_id\"] = gene_id\n",
    "                            element[\"start\"] = int(subsubf.location.start)\n",
    "                            element[\"end\"] = int(subsubf.location.end)\n",
    "                            element[\"strand\"] = \"+\" if subsubf.location.strand == 1 else \"-\"  # Extracting strand (adapted from original script to later be able to look at the organization of the genes)\n",
    "                            element[\"gene_spec_id\"] = gene_spec_id\n",
    "                            \n",
    "                            # element[\"seq\"] = str(subf.extract(rec.seq))\n",
    "                            if gene_id in record_dict:\n",
    "                                record_dict[gene_id].append(element)\n",
    "                            else:\n",
    "                                record_dict[gene_id] = [element]\n",
    "\n",
    "    handle.close()\n",
    "\n",
    "#pprint(record_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read sequence classes for all species\n",
    "seq_class_dicts = {}\n",
    "for species in species_list:\n",
    "\n",
    "    sequence_class_file = f\"{data_dir}/AdditionalFile2-SeqClasses.tsv\"\n",
    "    seq_class_dicts[species] = []\n",
    "    with open(sequence_class_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if line[0] == species_to_sequence_spec[species]:\n",
    "                if line[2] == \"chrX\":\n",
    "                    seq_class_dicts[species].append(line[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_class_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the protein sequences based on the protein file of the reference genome -> extract the protein ID name and its sequence\n",
    "prot_dicts = {}\n",
    "\n",
    "for species in species_list:\n",
    "\n",
    "    sequence_file = [d for d in data if d['species'] == species][0]['data']['prot']\n",
    "    sequence_handle = open(sequence_file)\n",
    "    seq_dict = SeqIO.to_dict(SeqIO.parse(sequence_handle, \"fasta\"))\n",
    "\n",
    "    prot_dicts[species] = seq_dict\n",
    "\n",
    "    sequence_handle.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect protein sequence for each gene -> match the protein ID in the record_dict dictionary (contains information about genes) to the protein ID and its sequence. It then appends the sequence\n",
    "for record in record_dict:\n",
    "    \n",
    "    for element in record_dict[record]:\n",
    "        species = element[\"specie\"]\n",
    "        id = element[\"id\"]\n",
    "        name = element[\"name\"]\n",
    "        # for prot in prot_dicts[specie]:\n",
    "        #     print(prot)\n",
    "        #     print(prot_dicts[specie][prot])\n",
    "        #     assert False\n",
    "        prot_sequences = [prot_dicts[species][x] for x in prot_dicts[species] if (id in x )]\n",
    "        if len(prot_sequences) == 0:\n",
    "            print(\"No sequence found\")\n",
    "            print(id)\n",
    "            assert False\n",
    "        seq = str(prot_sequences[0].seq)\n",
    "        if len(seq) > 1:\n",
    "            for prot in prot_sequences:\n",
    "                if len(prot.seq) > len(seq):\n",
    "                    seq = str(prot.seq)\n",
    "\n",
    "        element[\"seq\"] = seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce list of prot sequences to longest sequence per gene\n",
    "gene_record_dict = {}\n",
    "for record in record_dict:\n",
    "    gene_spec_ids = set()\n",
    "    gene_spec_elements = []\n",
    "    \n",
    "    for element in record_dict[record]:\n",
    "        # gene_id = element[\"gene_id\"]\n",
    "        gene_spec_id = element[\"gene_spec_id\"]\n",
    "        \n",
    "        if gene_spec_id not in gene_spec_ids:\n",
    "            gene_spec_elements.append(element)\n",
    "        else:\n",
    "            if len(element[\"seq\"]) > len([x for x in gene_spec_elements if x[\"gene_spec_id\"] == gene_spec_id][0][\"seq\"]):\n",
    "                for el in gene_spec_elements:\n",
    "                    if el[\"gene_spec_id\"] == gene_spec_id:\n",
    "                        el[\"seq\"] = element[\"seq\"]\n",
    "                        break\n",
    "\n",
    "        gene_spec_ids.add(element[\"gene_spec_id\"])\n",
    "    \n",
    "    gene_record_dict[record] = gene_spec_elements\n",
    "\n",
    "#pprint(gene_record_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print one protein file per specie\n",
    "cmd = f\"mkdir -p {work_dir}/protein_extracted_longest\"\n",
    "subprocess.run(cmd, shell=True)\n",
    "for species in species_list:\n",
    "    file = f\"{work_dir}/protein_extracted_longest/{species}.faa\"\n",
    "\n",
    "\n",
    "    species_records = {}\n",
    "    for gene in gene_record_dict:\n",
    "        species_records[gene] = []\n",
    "        for element in gene_record_dict[gene]:\n",
    "            if element[\"specie\"] == species:\n",
    "                species_records[gene].append(element)\n",
    "\n",
    "\n",
    "    with open(file, \"w\") as outfile:\n",
    "        for entry in species_records:\n",
    "            element = species_records[entry]\n",
    "            if len(element) > 1:\n",
    "                print(species)\n",
    "                assert False\n",
    "            if len(element) == 0:\n",
    "                continue\n",
    "            element = element[0]\n",
    "            outfile.write(f\">{element['id']} gene:{element['gene_id']} mrna:{element['mrna_id']} {element['specie']} {element['name']}\\n\")\n",
    "            outfile.write(f\"{element['seq']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a BLAST database from the protein files generate above. \n",
    "cmd = f\"mkdir -p {work_dir}/protein_extracted_longest/blastdb\" #create the directory\n",
    "subprocess.run(cmd, shell=True, check=True) #execute the command\n",
    "\n",
    "#concatanate all fasta files into one\n",
    "cmd = f\"cat {work_dir}/protein_extracted_longest/*.faa \\\n",
    "    > {work_dir}/protein_extracted_longest/blastdb/all_proteins.faa\" #matches all the .faa files (protein files for all species in the protein_extracted_longest foder\n",
    "# run command \n",
    "subprocess.run(cmd, shell=True, check=True) #execute the command. A concanated FASTA file is created -> contains all protein sequences from the .faa files created earlier, with their respective headers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the makeblastbd command to create a BLAST database from the concatenated FASTA file of protein sequences. \n",
    "cmd = f\"makeblastdb -in {work_dir}/protein_extracted_longest/blastdb/all_proteins.faa \\\n",
    "     -dbtype prot\\\n",
    "     -out {work_dir}/protein_extracted_longest/blastdb/all_proteins\"\n",
    "\n",
    "subprocess.run(cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a BLAST search for each species' protein sequences against the BLAST database created. \n",
    "# blast all against DB\n",
    "for s in species_list:\n",
    "    cmd = f\"blastp -query {work_dir}/protein_extracted_longest/{s}.faa \\\n",
    "    -db {work_dir}/protein_extracted_longest/blastdb/all_proteins \\\n",
    "    -out {work_dir}/protein_extracted_longest/{s}.blastp.tsv \\\n",
    "    -outfmt \\\"6 qseqid sseqid pident mismatch gapopen gaps qcovs qcovshsp evalue\\\" \"\n",
    "    subprocess.run(cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect edges for each protein\n",
    "edges = {}\n",
    "identity = 50\n",
    "coverage = 35\n",
    "score = 0.001 \n",
    "\n",
    "for s in species_list:\n",
    "    file = f\"{work_dir}/protein_extracted_longest/{s}.blastp.tsv\"\n",
    "    with open(file, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if float(line[2]) >= identity and int(line[6]) >= coverage and float(line[7]) < score:\n",
    "                if line[0] in edges:\n",
    "                    edges[line[0]].append(line[1])\n",
    "                else:\n",
    "                    edges[line[0]] = [line[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep two way edges\n",
    "edges_2way = []\n",
    "vertices = set()\n",
    "for node_A in edges:\n",
    "    for node_B in edges[node_A]:\n",
    "        if node_B in edges and node_A in edges[node_B]:\n",
    "            tuple = (node_A, node_B)\n",
    "            tuple = sorted(tuple)\n",
    "\n",
    "            if tuple in edges_2way:\n",
    "                continue\n",
    "\n",
    "            edges_2way.append(tuple)\n",
    "            vertices.add(node_A)\n",
    "            vertices.add(node_B)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clusters with transitive clusering\n",
    "clusters = []\n",
    "for edge in edges_2way:\n",
    "    found = False\n",
    "    for cluster in clusters:\n",
    "        if edge[0] in cluster or edge[1] in cluster:\n",
    "            cluster.add(edge[0])\n",
    "            cluster.add(edge[1])\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        clusters.append(set(edge))\n",
    "\n",
    "\n",
    "#merge clusters with common elements\n",
    "merged_clusters = []\n",
    "for cluster in clusters:\n",
    "    found = False\n",
    "    for merged_cluster in merged_clusters:\n",
    "        if len(cluster.intersection(merged_cluster)) > 0:\n",
    "            merged_cluster.update(cluster)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        merged_clusters.append(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link every protein to a their original species\n",
    "prot_to_species = {}\n",
    "\n",
    "for cluster in merged_clusters:\n",
    "\n",
    "    for prot in cluster:\n",
    "        # print(prot)\n",
    "        species = \"\"\n",
    "        for gene in gene_record_dict:\n",
    "            for element in gene_record_dict[gene]:\n",
    "                if element[\"id\"] == prot:\n",
    "                    species = element[\"specie\"]\n",
    "                    prot_to_species[prot] = species\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read blast results into pandas dataframe\n",
    "# columns :species, gene_A, gene_B, identity, coverage, evalue\n",
    "\n",
    "data_frames = {}\n",
    "for species in species_list:\n",
    "    blast_results = pd.DataFrame(columns=[\"gene_A\", \"gene_B\", \"identity\", \"coverage\", \"evalue\"]) \n",
    "    blast_results_list = []\n",
    "    file = f\"{work_dir}/protein_extracted_longest/{species}.blastp.tsv\"\n",
    "    with open(file, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            # if float(line[2]) >= identity and int(line[6]) >= coverage and float(line[7]) < score:\n",
    "            blast_results_list.append({\"species\": species, \"gene_A\": line[0], \"gene_B\": line[1], \"identity\": line[2], \"coverage\": line[6], \"evalue\": line[7]})\n",
    "\n",
    "    blast_results = pd.DataFrame(blast_results_list)\n",
    "    data_frames[species] = blast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palindrome_locations_file_path = \"./PalindromeLocations.tsv\" #this file is distributed along the code\n",
    "species_trans = {\n",
    "    \"HomSap_x\": \"chm13\"    ,\n",
    "    \"HomSap_y\": \"hg002\"    ,\n",
    "    \"GorGor\": \"mGorGor1\" ,\n",
    "    \"PanTro\": \"mPanTro3\" ,\n",
    "    \"PanPan\": \"mPanPan1\" ,\n",
    "    \"PonAbe\": \"mPonAbe1\" ,\n",
    "    \"PonPyg\": \"mPonPyg2\" ,\n",
    "    \"SymSyn\": \"mSymSyn1\" \n",
    "}\n",
    "palindrome_locations_file = open(palindrome_locations_file_path, \"r\")\n",
    "palindrome_locations = palindrome_locations_file.readlines()\n",
    "palindrome_locations_file.close()\n",
    "chr = \"x\"\n",
    "\n",
    "palindrome_lines_per_species = {}\n",
    "\n",
    "for species in species_list:\n",
    "    # Build the list of translation keys we actually have (HomSap is divided into X and Y so this should be taken into account):\n",
    "    if species == \"HomSap\":\n",
    "        raw_keys = [f\"{species}_x\", f\"{species}_y\"]\n",
    "    else:\n",
    "        raw_keys = [species]\n",
    "    # Only keep keys that exist in species_trans. Check if species exists in species_trans -> BECAUSE MacFas does not have palindrome information so this should be empty\n",
    "\n",
    "    keys = [k for k in raw_keys if k in species_trans]\n",
    "\n",
    "    # If none of those keys are in species_trans, just give an empty list and skip\n",
    "    if not keys:\n",
    "        palindrome_lines_per_species[species] = []\n",
    "        continue\n",
    "        \n",
    "    palindrome_lines = []\n",
    "        \n",
    "    for row in palindrome_locations:\n",
    "        row = row.strip().split(\"\\t\")\n",
    "        if len(row) < 4:\n",
    "            continue\n",
    "        if row[1] != '' and row[1] != 'start':\n",
    "                    \n",
    "            if species == \"HomSap\":\n",
    "                sp = f\"{species}_{chr}\"\n",
    "                species_chr = f\"{species_trans[sp]}.chr{chr.upper()}\"\n",
    "            else:\n",
    "                species_chr = f\"{species_trans[species]}.chr{chr.upper()}\"\n",
    "\n",
    "            if row[0] == species_chr:\n",
    "                palindrome_lines.append([x.replace(',','') for x in row])\n",
    "    palindrome_lines_per_species[species] = palindrome_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palindrome_lines_per_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory as a shell command\n",
    "cmd = f\"mkdir -p {work_dir}/protein_extracted_longest/clusters_merged\"\n",
    "subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "file = open(f\"{work_dir}/protein_extracted_longest/clusters_merged/clusters.tsv\", \"w\")\n",
    "file_identities = open(f\"{work_dir}/protein_extracted_longest/clusters_merged/clusters_identities.tsv\", \"w\")\n",
    "file_classes = open(f\"{work_dir}/protein_extracted_longest/clusters_merged/clusters_classes.tsv\", \"w\")\n",
    " \n",
    "header = \"id\\tgene_symbol\\tDescription(s)\"\n",
    "for species in species_list:\n",
    "    header += f\"\\t{species}\"\n",
    "\n",
    "print(header, file=file)\n",
    "print(header, file=file_identities)\n",
    "print(header, file=file_classes)\n",
    "\n",
    "counter = 0\n",
    "prot_to_cluster = {}\n",
    "list_of_all_identities = []\n",
    "\n",
    "for cluster in merged_clusters:\n",
    "\n",
    "    cluster_copy = copy.deepcopy(cluster)\n",
    "    counter += 1\n",
    "    species_count = {}\n",
    "    gene_names = set()\n",
    "    gene_symbol = \"\"\n",
    "    report = False\n",
    "    gene_identity_counter_per_species = {}\n",
    "\n",
    "    #class counter intialization -> so initialize counters for sequences classes (PAR, ampliconic, ancestral, palindrome), per species\n",
    "    classes_per_species = {}\n",
    "    for species in species_list:\n",
    "        classes_per_species[species] = {\n",
    "                \"par_counter\" : 0,\n",
    "                \"ampl_counter\" : 0,\n",
    "                \"ancestral_counter\" : 0,\n",
    "                \"palindrome_counter\" : 0\n",
    "        }\n",
    "    \n",
    "    # create cluster FASTA file \n",
    "    # write sequences for each cluster into a seperate FASTA file for further analysis\n",
    "    # then match proteins to their repsective genes using gene_record_dict and extract relevant information (species, description, sequence) \n",
    "    # HERE THERE SHOULD BE START AND END coordinates??\n",
    "    cluster_fasta_file = f\"{work_dir}/protein_extracted_longest/clusters_merged/cluster_{counter}.faa\"\n",
    "    with open(cluster_fasta_file, \"w\") as cluster_outfile:\n",
    "        for prot in cluster:\n",
    "            # print(prot)\n",
    "            species = \"\"\n",
    "            desc = \"\"\n",
    "            for gene in gene_record_dict:\n",
    "                for element in gene_record_dict[gene]:\n",
    "                    if element[\"id\"] == prot:\n",
    "                        #print(\"Element dictionary:\", element)\n",
    "                        gene_names.add(element[\"name\"])\n",
    "                        species = element[\"specie\"]\n",
    "                        # print(element)\n",
    "                        desc = element[\"name\"]\n",
    "                        report_gene = gene\n",
    "\n",
    "                        prot_to_cluster[prot] = counter\n",
    "                        \n",
    "                        if species in species_count:\n",
    "                            report = True\n",
    "                            # species = element\n",
    "                            species_count[species] += 1\n",
    "                        else:\n",
    "                            species_count[species] = 1\n",
    "                        sequence = element[\"seq\"]\n",
    "\n",
    "                        el_start =  element[\"start\"]\n",
    "                        el_end =  element[\"end\"]\n",
    "                        \n",
    "                        #Determine which class (PAR, ampliconic, ancestral) the sequence belongs to, based on genomic coordinates. SO HERE IT USES COORDINATES?!\n",
    "                        for seq_class in seq_class_dicts[species]:\n",
    "                            class_name = seq_class[3]\n",
    "                            if el_start >= int(seq_class[1]) and el_end <= int(seq_class[2]):\n",
    "\n",
    "                                if seq_class[3] == \"PAR\":\n",
    "                                    classes_per_species[species][\"par_counter\"] += 1\n",
    "                                elif seq_class[3] == \"AMPLICONIC\":\n",
    "                                    classes_per_species[species][\"ampl_counter\"] += 1\n",
    "                                elif seq_class[3] == \"ANCESTRAL\" :\n",
    "                                    classes_per_species[species][\"ancestral_counter\"] += 1\n",
    "                                break\n",
    "                            else:\n",
    "                                #TODO if more than 50% of the sequence is in the class, count it as in the class\n",
    "                                \n",
    "                                if el_start < int(seq_class[1]) and el_end > int(seq_class[1]):\n",
    "                                    if int(seq_class[1]) - el_start > 0.5 * len(sequence):\n",
    "                                        if seq_class[3] == \"PAR\":\n",
    "                                            classes_per_species[species][\"par_counter\"] += 1\n",
    "                                        elif seq_class[3] == \"AMPLICONIC\":\n",
    "                                            classes_per_species[species][\"ampl_counter\"] += 1\n",
    "                                        elif seq_class[3] == \"ANCESTRAL\":\n",
    "                                            classes_per_species[species][\"ancestral_counter\"] += 1\n",
    "                                        break\n",
    "                                if el_start < int(seq_class[2]) and el_end > int(seq_class[2]):\n",
    "                                    if el_end - int(seq_class[2]) > 0.5 * len(sequence):\n",
    "                                        if seq_class[3] == \"PAR\":\n",
    "                                            classes_per_species[species][\"par_counter\"] += 1\n",
    "                                        elif seq_class[3] == \"AMPLICONIC\":\n",
    "                                            classes_per_species[species][\"ampl_counter\"] += 1\n",
    "                                        elif seq_class[3] == \"ANCESTRAL\":\n",
    "                                            classes_per_species[species][\"ancestral_counter\"] += 1\n",
    "                                        break\n",
    "\n",
    "                        # check if the sequence is within palindrome regions and count accordingly \n",
    "                        for palindrome in palindrome_lines_per_species[species]:\n",
    "                            if el_start >= int(palindrome[1]) and el_end <= int(palindrome[2]):\n",
    "                                if counter == 21:\n",
    "                                    print(palindrome)\n",
    "                                classes_per_species[species][\"palindrome_counter\"] += 1\n",
    "                                break\n",
    "                            else:\n",
    "                                if el_start < int(palindrome[1]) and el_end > int(palindrome[2]):\n",
    "                                    \n",
    "                                    print(f\"start: {el_start} end: {el_end} palindrome: {palindrome[0]} start: {palindrome[1]} end: {palindrome[2]}\")\n",
    "                                    assert False\n",
    "                                if el_start < int(palindrome[1]) and el_end > int(palindrome[2]):\n",
    "                                    print(f\"start: {el_start} end: {el_end} palindrome: {palindrome[0]} start: {palindrome[1]} end: {palindrome[2]}\")\n",
    "                                    assert False\n",
    "\n",
    "                        if element[\"specie\"] == \"HomSap\":\n",
    "                            if gene.startswith(\"LOC\"):\n",
    "                                continue\n",
    "                            gene_symbol = gene\n",
    "                        break\n",
    "            \n",
    "            print(f\">{prot}_gene:{report_gene}_species:{species} description:{desc}\\n{sequence}\", file=cluster_outfile)   \n",
    "            for prot_B in cluster_copy:\n",
    "                if prot == prot_B:\n",
    "                    continue\n",
    "                species = prot_to_species[prot]\n",
    "                if species != prot_to_species[prot_B]:\n",
    "                    continue\n",
    "                # blast results initiated in later cells, TODO: move to top\n",
    "                blast_results = data_frames[species]\n",
    "                blast_result = blast_results[(blast_results[\"gene_A\"] == prot) & (blast_results[\"gene_B\"] == prot_B)]\n",
    "                if blast_result.empty:\n",
    "                    continue\n",
    "\n",
    "                identity1 = blast_result.iloc[0][\"identity\"]\n",
    "\n",
    "                blast_result = blast_results[(blast_results[\"gene_A\"] == prot_B) & (blast_results[\"gene_B\"] == prot)]\n",
    "                if blast_result.empty:\n",
    "                    continue\n",
    "                identity2 = blast_result.iloc[0][\"identity\"]\n",
    "\n",
    "                if identity2 < identity1:\n",
    "                    identity1 = identity2\n",
    "                if species not in gene_identity_counter_per_species:\n",
    "                    gene_identity_counter_per_species[species] = f\"{identity1}\"\n",
    "                else:\n",
    "                    gene_identity_counter_per_species[species] += f\";{identity1}\"\n",
    "                list_of_all_identities.append(float(identity1))\n",
    "            cluster_copy.remove(prot)\n",
    "\n",
    "# once processing is complete, write the collected information (clusters, identities, classes) into respective file           \n",
    "    if report:\n",
    "        \n",
    "        if gene_symbol == \"\":\n",
    "            gene_symbol = list(gene_names)[0]\n",
    "        line = f\"{counter}\\t{gene_symbol}\\t{';'.join(gene_names)}\"\n",
    "        for species in species_list:\n",
    "            if species not in species_count:\n",
    "                species_count[species] = 0\n",
    "            line += f\"\\t{species_count[species]}\"\n",
    "\n",
    "        print(line, file=file)\n",
    "\n",
    "        identity_line = f\"{counter}\\t{gene_symbol}\\t{';'.join(gene_names)}\"\n",
    "        for species in species_list:\n",
    "            if species not in gene_identity_counter_per_species:\n",
    "                gene_identity_counter_per_species[species] = 0\n",
    "            identity_line += f\"\\t{gene_identity_counter_per_species[species]}\"\n",
    "        print(identity_line, file=file_identities)\n",
    "\n",
    "        class_line = f\"{counter}\\t{gene_symbol}\\t{';'.join(gene_names)}\"\n",
    "        \n",
    "        per_line_class_counter = {\n",
    "            \"par_counter\" : 0 ,\n",
    "            \"ampl_counter\" : 0,\n",
    "            \"ancestral_counter\" : 0,\n",
    "            \"palindrome_counter\" : 0\n",
    "        }\n",
    "        for species in species_list:\n",
    "            par_counter = classes_per_species[species][\"par_counter\"]\n",
    "            per_line_class_counter[\"par_counter\"] += par_counter\n",
    "            ampl_counter = classes_per_species[species][\"ampl_counter\"]\n",
    "            per_line_class_counter[\"ampl_counter\"] += ampl_counter\n",
    "            ancestral_counter = classes_per_species[species][\"ancestral_counter\"]\n",
    "            per_line_class_counter[\"ancestral_counter\"] += ancestral_counter\n",
    "            palindrome_counter = classes_per_species[species][\"palindrome_counter\"]\n",
    "            per_line_class_counter[\"palindrome_counter\"] += palindrome_counter\n",
    "            class_line += f\"\\t{ampl_counter} ({palindrome_counter}) + {ancestral_counter} + {par_counter}\"\n",
    "\n",
    "\n",
    "        class_line += f\"\\t{per_line_class_counter['ampl_counter']} ({per_line_class_counter['palindrome_counter']}) + {per_line_class_counter['ancestral_counter']} + {per_line_class_counter['par_counter']}\"\n",
    "        print(class_line, file=file_classes)\n",
    "\n",
    "#close files\n",
    "file.close()\n",
    "file_identities.close()\n",
    "file_classes.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the genes coordinates and which classification they belong to. \n",
    "output_dir = f\"{work_dir}/protein_extracted_longest/clusters_merged\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "gene_details_file_path = os.path.join(output_dir, \"gene_details.tsv\")\n",
    "\n",
    "with open(gene_details_file_path, \"w\") as gene_details_file:\n",
    "    # Write header to the file\n",
    "    print(\"Species\\tGene\\tGene_symbol\\tStart\\tEnd\\tStrand\\tClass\", file=gene_details_file)\n",
    "\n",
    "    # Iterate over each species and their corresponding genes\n",
    "    for species in species_list:\n",
    "        for gene in gene_record_dict:\n",
    "            for element in gene_record_dict[gene]:\n",
    "                if element[\"specie\"] == species:\n",
    "                    gene_name = element[\"name\"]\n",
    "                    gene_symbol = gene\n",
    "                    start = element[\"start\"]\n",
    "                    end = element[\"end\"]\n",
    "                    sequence_class = \"Unknown\"\n",
    "                    strand = element[\"strand\"]\n",
    "\n",
    "                    # Determine the sequence class\n",
    "                    for seq_class in seq_class_dicts[species]:\n",
    "                        if start >= int(seq_class[1]) and end <= int(seq_class[2]):\n",
    "                            sequence_class = seq_class[3]\n",
    "                            break\n",
    "\n",
    "                    # Print gene details to the file\n",
    "                    print(f\"{species}\\t{gene_name}\\t{gene_symbol}\\t{start}\\t{end}\\t{strand}\\t{sequence_class}\", file=gene_details_file)\n",
    "\n",
    "print(f\"Gene details have been saved to {gene_details_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the ampliconic gene families \n",
    "# Defined as sequence identity for at least two copies have ≥97% sequence identity within at least one species.\n",
    "\n",
    "# Load the Cluster identities TSV file\n",
    "df = pd.read_csv(f\"{work_dir}/protein_extracted_longest/clusters_merged/clusters_identities.tsv\", sep='\\t')\n",
    "\n",
    "# Check if at least 2 copies have >= 97% identity\n",
    "def is_ampliconic(sequence_identities):\n",
    "    # Split the string by semicolons and convert to floats\n",
    "    identities = [float(identity) for identity in sequence_identities.split(';') if identity]\n",
    "    \n",
    "    # Check if at least one species has 2 or more identities >= 97%\n",
    "    return sum([identity >= 97.0 for identity in identities]) >= 1\n",
    "\n",
    "# Filter the rows based on the condition for PanTro, HomSap, and PanPan\n",
    "filtered_rows = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if is_ampliconic(row['PanTro']) or is_ampliconic(row['HomSap']) or is_ampliconic(row['PanPan']) or is_ampliconic(row['GorGor']) or is_ampliconic(row['PonPyg']) or is_ampliconic(row['PonAbe']) or is_ampliconic(row['SymSyn']):\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "# Create a new dataframe with only the ampliconic families\n",
    "ampliconic_df = pd.DataFrame(filtered_rows)\n",
    "\n",
    "# Save the filtered dataframe to a new TSV file\n",
    "ampliconic_df.to_csv(f\"{work_dir}/protein_extracted_longest/clusters_merged/ampliconic_families.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the gene family to the genes of the gene_details.tsv file \n",
    "# Load the gene_details and clusters files\n",
    "gene_details_file = f\"{work_dir}/protein_extracted_longest/clusters_merged/gene_details.tsv\"\n",
    "clusters_file = f\"{work_dir}/protein_extracted_longest/clusters_merged/ampliconic_families.tsv\"\n",
    "\n",
    "gene_details_df = pd.read_csv(gene_details_file, sep='\\t')\n",
    "clusters_df = pd.read_csv(clusters_file, sep='\\t')\n",
    "\n",
    "gene_details_df[\"gene_family_symbol\"] = \"\"\n",
    "\n",
    "# Iterate through each row in gene_details_df\n",
    "for index, row in gene_details_df.iterrows():\n",
    "    gene_name = row[\"Gene\"]\n",
    "    \n",
    "    # Search for the gene_name in the \"Description(s)\" column of clusters_df\n",
    "    matching_row = clusters_df[clusters_df[\"Description(s)\"].str.contains(gene_name, na=False, regex=False)]\n",
    "    \n",
    "    if not matching_row.empty:\n",
    "        # Get the gene_symbol from the matching row\n",
    "        gene_symbol = matching_row.iloc[0][\"gene_symbol\"]\n",
    "        # Append the gene_symbol to the \"gene_family_symbol\" column in gene_details_df\n",
    "        gene_details_df.at[index, \"gene_family_symbol\"] = gene_symbol\n",
    "\n",
    "# Save the updated DataFrame back to a new file\n",
    "gene_details_df.to_csv(f\"{work_dir}/protein_extracted_longest/clusters_merged/gene_details_updated.tsv\", sep='\\t', index=False)\n",
    "\n",
    "print(\"Updated gene_details.tsv with gene_family_symbol column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin list of identities by values 50 to 55, 55 to 60, 60 to 65, 65 to 70, 70 to 75, 75 to 80, 80 to 85, 85 to 90, 90 to 95, 95 to 100\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.hist(list_of_all_identities, bins = 50, color = 'Pink', edgecolor='black', alpha = 0.7)\n",
    "# plt.yscale('log')\n",
    "plt.xlabel('Identity', fontsize = 12)\n",
    "plt.ylabel('Count', fontsize = 12)\n",
    "plt.title('Histogram of identities', fontsize = 14)\n",
    "\n",
    "#remove frame and white space around plot\n",
    "sns.despine()\n",
    "# Add a black dashed vertical line at x = 97\n",
    "plt.axvline(x=97, color='black', linestyle='--')\n",
    "\n",
    "plt.savefig(f\"{work_dir}/protein_extracted_longest/clusters_merged/identity_histogram.\")\n",
    "# plt.yscale('log')\n",
    "# plt.savefig(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/identity_histogram_log.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply whether a gene lays within a palindrome\n",
    "gene_details_df = pd.read_csv(f\"{work_dir}/protein_extracted_longest/clusters_merged/gene_details_updated.tsv\", sep='\\t')\n",
    "\n",
    "## Check if a gene is within any palindrome\n",
    "def check_palindrome(specie, gene_start, gene_end):\n",
    "    # Get the list of palindromes for the given species\n",
    "    if specie in palindrome_lines_per_species:\n",
    "        palindromes = palindrome_lines_per_species[specie]\n",
    "        \n",
    "        for palindrome in palindromes:\n",
    "            _, pal_start, pal_end, _, pal_name = palindrome\n",
    "            \n",
    "            # Convert coordinates to integers\n",
    "            pal_start = int(pal_start)\n",
    "            pal_end = int(pal_end)\n",
    "            \n",
    "            # Check if gene coordinates fall within the palindrome range\n",
    "            if gene_start >= pal_start and gene_end <= pal_end:\n",
    "                return 'yes', pal_name\n",
    "    \n",
    "    return 'no', None\n",
    "    \n",
    "# New columns to be added\n",
    "gene_details_df['in_palindrome'] = 'no'\n",
    "gene_details_df['palindrome_name'] = None\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for idx, row in gene_details_df.iterrows():\n",
    "    species = row['Species']  # Assuming column named 'specie'\n",
    "    gene_start = row['Start']  # Assuming column named 'start'\n",
    "    gene_end = row['End']  # Assuming column named 'end'\n",
    "    \n",
    "    in_palindrome, palindrome_name = check_palindrome(species, gene_start, gene_end)\n",
    "    \n",
    "    # Update the dataframe with the results\n",
    "    gene_details_df.at[idx, 'in_palindrome'] = in_palindrome\n",
    "    gene_details_df.at[idx, 'palindrome_name'] = palindrome_name\n",
    "\n",
    "# Save the updated dataframe to a new file\n",
    "gene_details_df.to_csv(f\"{work_dir}/protein_extracted_longest/clusters_merged/gene_details_updated_with_palindromes.tsv\", sep='\\t', index=False)\n",
    "\n",
    "gene_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE THE GENE COORDINATES! they are not the full gene length now\n",
    "gene_details_file = f\"{work_dir}/protein_extracted_longest/clusters_merged/gene_details_updated_with_palindromes.tsv\"\n",
    "df = pd.read_csv(gene_details_file, sep=\"\\t\")\n",
    "df\n",
    "\n",
    "species_to_gff = {\n",
    "    d['species']: d['data']['path_to_annotation_NCBI'].replace(\".gff\", \"_chrX.gff\")\n",
    "    for d in data\n",
    "}\n",
    "\n",
    "def parse_gene_coords(gff_path):\n",
    "    coords = {}\n",
    "    with open(gff_path) as g:\n",
    "        for line in g:\n",
    "            if line.startswith('#'): continue\n",
    "            chrom,src,feat,start,end,_,_,_,attr = line.split('\\t',8)\n",
    "            if feat != 'gene': continue\n",
    "            # pull out gene_symbol (or fallback to gene=)\n",
    "            md = dict(item.split('=',1) for item in attr.split(';') if '=' in item)\n",
    "            sym = md.get('gene_symbol') or md.get('gene')\n",
    "            if sym:\n",
    "                coords[sym] = (int(start), int(end))\n",
    "    return coords\n",
    "\n",
    "gff_cache = {\n",
    "    sp: parse_gene_coords(path)\n",
    "    for sp, path in species_to_gff.items()\n",
    "}\n",
    "\n",
    "# replace Start/End \n",
    "new_starts, new_ends = [], []\n",
    "for _, row in df.iterrows():\n",
    "    sp, sym = row['Species'], row['Gene_symbol']\n",
    "    start,end = gff_cache[sp].get(sym, (row['Start'], row['End']))\n",
    "    new_starts.append(start)\n",
    "    new_ends.append(end)\n",
    "\n",
    "df['Start'] = new_starts\n",
    "df['End']   = new_ends\n",
    "df\n",
    "\n",
    "# save the table \n",
    "df.to_csv(f\"{work_dir}/protein_extracted_longest/clusters_merged/gene_details_updated_with_palindromes_coordinates.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows that have NaN in gene_family_symbol -> all the ones that are not part of a ampliconic gene family\n",
    "df = df.dropna(subset=[\"gene_family_symbol\"])\n",
    "df.to_csv(f\"{work_dir}/protein_extracted_longest/clusters_merged/gene_details_updated_with_palindromes_coordinates.tsv\", sep=\"\\t\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y-Chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs around 20s without extracting the sequence\n",
    "#runs around 1m with extracting the sequence\n",
    "#only collect ID's will extract sequence from cds_from_genomic.fna\n",
    "record_dict = {}\n",
    "\n",
    "for species in species_list:\n",
    "\n",
    "\n",
    "    annotation_file =  [d for d in data if d['species'] == species][0]['data']['path_to_annotation_NCBI'].replace(\".gff\", \"_chrY.gff\")\n",
    "    handle = open(annotation_file)\n",
    "\n",
    "    # pprint.pprint(examiner.parent_child_map(handle))\n",
    "\n",
    "    # assert False\n",
    "\n",
    "    # records = GFF.parse(handle,base_dict=seq_dict)\n",
    "    records = GFF.parse(handle)\n",
    "    for rec in records:\n",
    "        protss = set()\n",
    "        for feature in rec.features:\n",
    "            if feature.type != \"gene\":\n",
    "                continue\n",
    "            if \"description\" in feature.qualifiers:\n",
    "                description = feature.qualifiers[\"description\"][0]\n",
    "\n",
    "                gene_id = feature.id.replace(\"gene-\", \"\")\n",
    "                gene_spec_id = gene_id + \"_\" + species\n",
    "                    \n",
    "                # print(feature.qualifiers)\n",
    "                for subf in feature.sub_features:\n",
    "\n",
    "                    if subf.type == \"mRNA\":\n",
    "                        mrna_id = subf.id.replace(\"rna-\", \"\")\n",
    "                        for subsubf in subf.sub_features:\n",
    "                            if subsubf.type != \"CDS\":\n",
    "                                continue\n",
    "                            # cds_id = subsubf.id.replace(\"cds-\", \"\")\n",
    "                            prot_id = subsubf.qualifiers[\"protein_id\"][0]\n",
    "                            if prot_id in protss:\n",
    "                                continue\n",
    "\n",
    "                            protss.add(prot_id)\n",
    "                            element = {}\n",
    "                            element[\"specie\"] = species\n",
    "                            element[\"name\"] = description\n",
    "                            element[\"id\"] = prot_id\n",
    "                            element[\"mrna_id\"] = mrna_id\n",
    "                            element[\"gene_id\"] = gene_id\n",
    "                            element[\"start\"] = int(subsubf.location.start)\n",
    "                            element[\"end\"] = int(subsubf.location.end)\n",
    "                            element[\"strand\"] = \"+\" if subsubf.location.strand == 1 else \"-\"  # Extracting strand (adapted from original script to later be able to look at the organization of the genes)\n",
    "                            element[\"gene_spec_id\"] = gene_spec_id\n",
    "\n",
    "                            # element[\"seq\"] = str(subf.extract(rec.seq))\n",
    "                            if gene_id in record_dict:\n",
    "                                record_dict[gene_id].append(element)\n",
    "                            else:\n",
    "                                record_dict[gene_id] = [element]\n",
    "\n",
    "    handle.close()\n",
    "\n",
    "#pprint(record_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read sequence classes for all species\n",
    "seq_class_dicts = {}\n",
    "for species in species_list:\n",
    "\n",
    "    sequence_class_file = f\"{data_dir}/AdditionalFile2-SeqClasses.tsv\"\n",
    "    seq_class_dicts[species] = []\n",
    "    with open(sequence_class_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if line[0] == species_to_sequence_spec[species]:\n",
    "                if line[2] == \"chrY\":\n",
    "                    seq_class_dicts[species].append(line[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(seq_class_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the cds_from_genomic.fna file\n",
    "prot_dicts = {}\n",
    "\n",
    "for species in species_list:\n",
    "\n",
    "    sequence_file = [d for d in data if d['species'] == species][0]['data']['prot']\n",
    "    sequence_handle = open(sequence_file)\n",
    "    seq_dict = SeqIO.to_dict(SeqIO.parse(sequence_handle, \"fasta\"))\n",
    "\n",
    "    prot_dicts[species] = seq_dict\n",
    "\n",
    "    sequence_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in record_dict:\n",
    "    \n",
    "    for element in record_dict[record]:\n",
    "        species = element[\"specie\"]\n",
    "        id = element[\"id\"]\n",
    "        name = element[\"name\"]\n",
    "        prot_sequences = [prot_dicts[species][x] for x in prot_dicts[species] if (id in x )]\n",
    "        if len(prot_sequences) == 0:\n",
    "            print(\"No sequence found\")\n",
    "            print(id)\n",
    "            assert False\n",
    "        seq = str(prot_sequences[0].seq)\n",
    "        if len(seq) > 1:\n",
    "            for prot in prot_sequences:\n",
    "                if len(prot.seq) > len(seq):\n",
    "                    seq = str(prot.seq)\n",
    "\n",
    "        element[\"seq\"] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce list of prot sequences to longest sequence per gene\n",
    "gene_record_dict = {}\n",
    "for record in record_dict:\n",
    "    gene_spec_ids = set()\n",
    "    gene_spec_elements = []\n",
    "    \n",
    "    for element in record_dict[record]:\n",
    "        # gene_id = element[\"gene_id\"]\n",
    "        gene_spec_id = element[\"gene_spec_id\"]\n",
    "        \n",
    "        if gene_spec_id not in gene_spec_ids:\n",
    "            gene_spec_elements.append(element)\n",
    "        else:\n",
    "            if len(element[\"seq\"]) > len([x for x in gene_spec_elements if x[\"gene_spec_id\"] == gene_spec_id][0][\"seq\"]):\n",
    "                for el in gene_spec_elements:\n",
    "                    if el[\"gene_spec_id\"] == gene_spec_id:\n",
    "                        el[\"seq\"] = element[\"seq\"]\n",
    "                        break\n",
    "\n",
    "        gene_spec_ids.add(element[\"gene_spec_id\"])\n",
    "    \n",
    "    gene_record_dict[record] = gene_spec_elements\n",
    "\n",
    "#pprint(gene_record_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print one protein file per specie\n",
    "for species in species_list:\n",
    "    file = f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/{species}.faa\"\n",
    "\n",
    "\n",
    "    species_records = {}\n",
    "    for gene in gene_record_dict:\n",
    "        species_records[gene] = []\n",
    "        for element in gene_record_dict[gene]:\n",
    "            if element[\"specie\"] == species:\n",
    "                species_records[gene].append(element)\n",
    "\n",
    "\n",
    "    with open(file, \"w\") as outfile:\n",
    "        for entry in species_records:\n",
    "            element = species_records[entry]\n",
    "            if len(element) > 1:\n",
    "                print(species)\n",
    "                assert False\n",
    "            if len(element) == 0:\n",
    "                continue\n",
    "            element = element[0]\n",
    "            outfile.write(f\">{element['id']} gene:{element['gene_id']} mrna:{element['mrna_id']} {element['specie']} {element['name']}\\n\")\n",
    "            outfile.write(f\"{element['seq']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make blast db\n",
    "cmd = f\"mkdir -p {work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/blastdb\"\n",
    "subprocess.run(cmd, shell=True, check=True)\n",
    "#concat all fasta files\n",
    "cmd = f\"cat {work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/*.faa \\\n",
    "    > {work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/blastdb/all_proteins.faa\"\n",
    "# run command\n",
    "subprocess.run(cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make blast db\n",
    "cmd = f\"makeblastdb -in {work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/blastdb/all_proteins.faa \\\n",
    "     -dbtype prot\\\n",
    "     -out {work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/blastdb/all_proteins\"\n",
    "\n",
    "subprocess.run(cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blast all against DB\n",
    "for s in species_list:\n",
    "    cmd = f\"blastp -query {work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/{s}.faa \\\n",
    "    -db {work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/blastdb/all_proteins \\\n",
    "    -out {work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/{s}.blastp.tsv \\\n",
    "    -outfmt \\\"6 qseqid sseqid pident mismatch gapopen gaps qcovs qcovshsp evalue\\\" \"\n",
    "    subprocess.run(cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect edges for each protein\n",
    "edges = {}\n",
    "identity = 50\n",
    "coverage = 35\n",
    "score = 0.001 \n",
    "\n",
    "for s in species_list:\n",
    "    file = f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/{s}.blastp.tsv\"\n",
    "    with open(file, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if float(line[2]) >= identity and int(line[6]) >= coverage and float(line[7]) < score:\n",
    "                if line[0] in edges:\n",
    "                    edges[line[0]].append(line[1])\n",
    "                else:\n",
    "                    edges[line[0]] = [line[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep two way edges\n",
    "edges_2way = []\n",
    "vertices = set()\n",
    "for node_A in edges:\n",
    "    for node_B in edges[node_A]:\n",
    "        if node_B in edges and node_A in edges[node_B]:\n",
    "            tuple = (node_A, node_B)\n",
    "            tuple = sorted(tuple)\n",
    "\n",
    "            if tuple in edges_2way:\n",
    "                continue\n",
    "\n",
    "            edges_2way.append(tuple)\n",
    "            vertices.add(node_A)\n",
    "            vertices.add(node_B)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clusters with transitive clusering\n",
    "clusters = []\n",
    "for edge in edges_2way:\n",
    "    found = False\n",
    "    for cluster in clusters:\n",
    "        if edge[0] in cluster or edge[1] in cluster:\n",
    "            cluster.add(edge[0])\n",
    "            cluster.add(edge[1])\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        clusters.append(set(edge))\n",
    "\n",
    "\n",
    "#merge clusters with common elements\n",
    "merged_clusters = []\n",
    "for cluster in clusters:\n",
    "    found = False\n",
    "    for merged_cluster in merged_clusters:\n",
    "        if len(cluster.intersection(merged_cluster)) > 0:\n",
    "            merged_cluster.update(cluster)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        merged_clusters.append(cluster)\n",
    "\n",
    "#merged_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_to_species = {}\n",
    "\n",
    "for cluster in merged_clusters:\n",
    "\n",
    "    for prot in cluster:\n",
    "        # print(prot)\n",
    "        species = \"\"\n",
    "        for gene in gene_record_dict:\n",
    "            for element in gene_record_dict[gene]:\n",
    "                if element[\"id\"] == prot:\n",
    "                    species = element[\"specie\"]\n",
    "                    prot_to_species[prot] = species\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read blast results into pandas dataframe\n",
    "# columns :species, gene_A, gene_B, identity, coverage, evalue\n",
    "\n",
    "\n",
    "\n",
    "data_frames = {}\n",
    "for species in species_list:\n",
    "    blast_results = pd.DataFrame(columns=[\"gene_A\", \"gene_B\", \"identity\", \"coverage\", \"evalue\"]) \n",
    "    blast_results_list = []\n",
    "    file = f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/{species}.blastp.tsv\"\n",
    "    with open(file, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            # if float(line[2]) >= identity and int(line[6]) >= coverage and float(line[7]) < score:\n",
    "            blast_results_list.append({\"species\": species, \"gene_A\": line[0], \"gene_B\": line[1], \"identity\": line[2], \"coverage\": line[6], \"evalue\": line[7]})\n",
    "\n",
    "    blast_results = pd.DataFrame(blast_results_list)\n",
    "    data_frames[species] = blast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palindrome_locations_file_path = \"./PalindromeLocations.tsv\" #this file is distributed along the code\n",
    "species_trans = {\n",
    "    \"HomSap_x\": \"chm13\"    ,\n",
    "    \"HomSap_y\": \"hg002\"    ,\n",
    "    \"GorGor\": \"mGorGor1\" ,\n",
    "    \"PanTro\": \"mPanTro3\" ,\n",
    "    \"PanPan\": \"mPanPan1\" ,\n",
    "    \"PonAbe\": \"mPonAbe1\" ,\n",
    "    \"PonPyg\": \"mPonPyg2\" ,\n",
    "    \"SymSyn\": \"mSymSyn1\" \n",
    "}\n",
    "palindrome_locations_file = open(palindrome_locations_file_path, \"r\")\n",
    "palindrome_locations = palindrome_locations_file.readlines()\n",
    "palindrome_locations_file.close()\n",
    "chr = \"y\"\n",
    "\n",
    "palindrome_lines_per_species = {}\n",
    "\n",
    "for species in species_list:\n",
    "    # Build the list of translation keys we actually have (HomSap is divided into X and Y so this should be taken into account):\n",
    "    if species == \"HomSap\":\n",
    "        raw_keys = [f\"{species}_x\", f\"{species}_y\"]\n",
    "    else:\n",
    "        raw_keys = [species]\n",
    "    # Only keep keys that exist in species_trans. Check if species exists in species_trans -> BECAUSE MacFas does not have palindrome information so this should be empty\n",
    "\n",
    "    keys = [k for k in raw_keys if k in species_trans]\n",
    "\n",
    "    # If none of those keys are in species_trans, just give an empty list and skip\n",
    "    if not keys:\n",
    "        palindrome_lines_per_species[species] = []\n",
    "        continue\n",
    "        \n",
    "    palindrome_lines = []\n",
    "        \n",
    "    for row in palindrome_locations:\n",
    "        row = row.strip().split(\"\\t\")\n",
    "        if len(row) < 4:\n",
    "            continue\n",
    "        if row[1] != '' and row[1] != 'start':\n",
    "                    \n",
    "            if species == \"HomSap\":\n",
    "                sp = f\"{species}_{chr}\"\n",
    "                species_chr = f\"{species_trans[sp]}.chr{chr.upper()}\"\n",
    "            else:\n",
    "                species_chr = f\"{species_trans[species]}.chr{chr.upper()}\"\n",
    "\n",
    "            if row[0] == species_chr:\n",
    "                palindrome_lines.append([x.replace(',','') for x in row])\n",
    "    palindrome_lines_per_species[species] = palindrome_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(palindrome_lines_per_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"mkdir -p {work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged\"\n",
    "subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "file = open(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/clusters.tsv\", \"w\")\n",
    "file_identities = open(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/clusters_identities.tsv\", \"w\")\n",
    "file_classes = open(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/clusters_classes.tsv\", \"w\")\n",
    "\n",
    "header = \"id\\tgene_symbol\\tDescription(s)\"\n",
    "for species in species_list:\n",
    "    header += f\"\\t{species}\"\n",
    "\n",
    "print(header, file=file)\n",
    "print(header, file=file_identities)\n",
    "print(header, file=file_classes)\n",
    "\n",
    "counter = 0\n",
    "prot_to_cluster = {}\n",
    "list_of_all_identities = []\n",
    "for cluster in merged_clusters:\n",
    "\n",
    "    cluster_copy = copy.deepcopy(cluster)\n",
    "    counter += 1\n",
    "    species_count = {}\n",
    "    gene_names = set()\n",
    "    gene_symbol = \"\"\n",
    "    report = False\n",
    "    gene_identity_counter_per_species = {}\n",
    "\n",
    "    classes_per_species = {}\n",
    "    for species in species_list:\n",
    "        classes_per_species[species] = {\n",
    "                \"par_counter\" : 0,\n",
    "                \"ampl_counter\" : 0,\n",
    "                \"ancestral_counter\" : 0,\n",
    "                \"palindrome_counter\" : 0\n",
    "        }\n",
    "    \n",
    "\n",
    "    cluster_fasta_file = f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/cluster_{counter}.faa\"\n",
    "    with open(cluster_fasta_file, \"w\") as cluster_outfile:\n",
    "        for prot in cluster:\n",
    "            # print(prot)\n",
    "            species = \"\"\n",
    "            desc = \"\"\n",
    "            for gene in gene_record_dict:\n",
    "                for element in gene_record_dict[gene]:\n",
    "                    if element[\"id\"] == prot:\n",
    "                        gene_names.add(element[\"name\"])\n",
    "                        species = element[\"specie\"]\n",
    "                        # print(element)\n",
    "                        desc = element[\"name\"]\n",
    "                        report_gene = gene\n",
    "\n",
    "                        prot_to_cluster[prot] = counter\n",
    "                        \n",
    "                        if species in species_count:\n",
    "                            report = True\n",
    "                            # species = element\n",
    "                            species_count[species] += 1\n",
    "                        else:\n",
    "                            species_count[species] = 1\n",
    "                        sequence = element[\"seq\"]\n",
    "\n",
    "                        el_start =  element[\"start\"]\n",
    "                        el_end =  element[\"end\"]\n",
    "\n",
    "                        for seq_class in seq_class_dicts[species]:\n",
    "                            class_name = seq_class[3]\n",
    "                            if el_start >= int(seq_class[1]) and el_end <= int(seq_class[2]):\n",
    "\n",
    "                                if seq_class[3] == \"PAR\":\n",
    "                                    classes_per_species[species][\"par_counter\"] += 1\n",
    "                                elif seq_class[3] == \"AMPLICONIC\":\n",
    "                                    classes_per_species[species][\"ampl_counter\"] += 1\n",
    "                                elif seq_class[3] == \"ANCESTRAL\":\n",
    "                                    classes_per_species[species][\"ancestral_counter\"] += 1\n",
    "                                break\n",
    "                            else:\n",
    "                               #TODO if more than 50% of the sequence is in the class, count it as in the class\n",
    "                                \n",
    "                                if el_start < int(seq_class[1]) and el_end > int(seq_class[1]):\n",
    "                                    if int(seq_class[1]) - el_start > 0.5 * len(sequence):\n",
    "                                        if seq_class[3] == \"PAR\":\n",
    "                                            classes_per_species[species][\"par_counter\"] += 1\n",
    "                                        elif seq_class[3] == \"AMPLICONIC\":\n",
    "                                            classes_per_species[species][\"ampl_counter\"] += 1\n",
    "                                        elif seq_class[3] == \"ANCESTRAL\":\n",
    "                                            classes_per_species[species][\"ancestral_counter\"] += 1\n",
    "                                        break\n",
    "                                if el_start < int(seq_class[2]) and el_end > int(seq_class[2]):\n",
    "                                    if el_end - int(seq_class[2]) > 0.5 * len(sequence):\n",
    "                                        if seq_class[3] == \"PAR\":\n",
    "                                            classes_per_species[species][\"par_counter\"] += 1\n",
    "                                        elif seq_class[3] == \"AMPLICONIC\":\n",
    "                                            classes_per_species[species][\"ampl_counter\"] += 1\n",
    "                                        elif seq_class[3] == \"ANCESTRAL\":\n",
    "                                            classes_per_species[species][\"ancestral_counter\"] += 1\n",
    "                                        break\n",
    "\n",
    "                        \n",
    "                        for palindrome in palindrome_lines_per_species[species]:\n",
    "                            if el_start >= int(palindrome[1]) and el_end <= int(palindrome[2]):\n",
    "                                classes_per_species[species][\"palindrome_counter\"] += 1\n",
    "                                break\n",
    "                            else:\n",
    "                                if el_start < int(palindrome[1]) and el_end > int(palindrome[2]):\n",
    "\n",
    "                                    print(f\"start: {el_start} end: {el_end} palindrome: {palindrome[0]} start: {palindrome[1]} end: {palindrome[2]}\")\n",
    "                                    assert False\n",
    "                                if el_start < int(palindrome[1]) and el_end > int(palindrome[2]):\n",
    "                                    if counter == 245:\n",
    "                                        print(palindrome)\n",
    "                                    print(f\"start: {el_start} end: {el_end} palindrome: {palindrome[0]} start: {palindrome[1]} end: {palindrome[2]}\")\n",
    "                                    assert False\n",
    "\n",
    "                        if element[\"specie\"] == \"HomSap\":\n",
    "                            if gene.startswith(\"LOC\"):\n",
    "                                continue\n",
    "                            gene_symbol = gene\n",
    "                        break\n",
    "            \n",
    "            print(f\">{prot}_gene:{report_gene}_species:{species} description:{desc}\\n{sequence}\", file=cluster_outfile)   \n",
    "            for prot_B in cluster_copy:\n",
    "                if prot == prot_B:\n",
    "                    continue\n",
    "                species = prot_to_species[prot]\n",
    "                if species != prot_to_species[prot_B]:\n",
    "                    continue\n",
    "                # blast results initiated in later cells, TODO: move to top\n",
    "                blast_results = data_frames[species]\n",
    "                blast_result = blast_results[(blast_results[\"gene_A\"] == prot) & (blast_results[\"gene_B\"] == prot_B)]\n",
    "                if blast_result.empty:\n",
    "                    continue\n",
    "\n",
    "                identity1 = blast_result.iloc[0][\"identity\"]\n",
    "\n",
    "                blast_result = blast_results[(blast_results[\"gene_A\"] == prot_B) & (blast_results[\"gene_B\"] == prot)]\n",
    "                if blast_result.empty:\n",
    "                    continue\n",
    "                identity2 = blast_result.iloc[0][\"identity\"]\n",
    "\n",
    "                if identity2 < identity1:\n",
    "                    identity1 = identity2\n",
    "                if species not in gene_identity_counter_per_species:\n",
    "                    gene_identity_counter_per_species[species] = f\"{identity1}\"\n",
    "                else:\n",
    "                    gene_identity_counter_per_species[species] += f\";{identity1}\"\n",
    "                list_of_all_identities.append(float(identity1))\n",
    "            cluster_copy.remove(prot)\n",
    "                \n",
    "    if report:\n",
    "        \n",
    "        if gene_symbol == \"\":\n",
    "            gene_symbol = list(gene_names)[0]\n",
    "        line = f\"{counter}\\t{gene_symbol}\\t{';'.join(gene_names)}\"\n",
    "        for species in species_list:\n",
    "            if species not in species_count:\n",
    "                species_count[species] = 0\n",
    "            line += f\"\\t{species_count[species]}\"\n",
    "\n",
    "        print(line, file=file)\n",
    "\n",
    "        identity_line = f\"{counter}\\t{gene_symbol}\\t{';'.join(gene_names)}\"\n",
    "        for species in species_list:\n",
    "            if species not in gene_identity_counter_per_species:\n",
    "                gene_identity_counter_per_species[species] = 0\n",
    "            identity_line += f\"\\t{gene_identity_counter_per_species[species]}\"\n",
    "        print(identity_line, file=file_identities)\n",
    "\n",
    "        class_line = f\"{counter}\\t{gene_symbol}\\t{';'.join(gene_names)}\"\n",
    "        \n",
    "        per_line_class_counter = {\n",
    "            \"par_counter\" : 0 ,\n",
    "            \"ampl_counter\" : 0,\n",
    "            \"ancestral_counter\" : 0,\n",
    "            \"palindrome_counter\" : 0\n",
    "        }\n",
    "        for species in species_list:\n",
    "            par_counter = classes_per_species[species][\"par_counter\"]\n",
    "            per_line_class_counter[\"par_counter\"] += par_counter\n",
    "            ampl_counter = classes_per_species[species][\"ampl_counter\"]\n",
    "            per_line_class_counter[\"ampl_counter\"] += ampl_counter\n",
    "            ancestral_counter = classes_per_species[species][\"ancestral_counter\"]\n",
    "            per_line_class_counter[\"ancestral_counter\"] += ancestral_counter\n",
    "            palindrome_counter = classes_per_species[species][\"palindrome_counter\"]\n",
    "            per_line_class_counter[\"palindrome_counter\"] += palindrome_counter\n",
    "            class_line += f\"\\t{ampl_counter} ({palindrome_counter}) + {ancestral_counter} + {par_counter}\"\n",
    "\n",
    "\n",
    "        class_line += f\"\\t{per_line_class_counter['ampl_counter']} ({per_line_class_counter['palindrome_counter']}) + {per_line_class_counter['ancestral_counter']} + {per_line_class_counter['par_counter']}\"\n",
    "        print(class_line, file=file_classes)\n",
    "     \n",
    "file.close()\n",
    "file_identities.close()\n",
    "file_classes.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the genes coordinates and which classification they belong to. \n",
    "output_dir = f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "gene_details_file_path = os.path.join(output_dir, \"gene_details.tsv\")\n",
    "\n",
    "# Open the file to save gene details\n",
    "with open(gene_details_file_path, \"w\") as gene_details_file:\n",
    "    # Write header to the file\n",
    "    print(\"Species\\tGene\\tGene_symbol\\tStart\\tEnd\\tStrand\\tClass\", file=gene_details_file)\n",
    "\n",
    "    # Iterate over each species and their corresponding genes\n",
    "    for species in species_list:\n",
    "        for gene in gene_record_dict:\n",
    "            for element in gene_record_dict[gene]:\n",
    "                if element[\"specie\"] == species:\n",
    "                    gene_name = element[\"name\"]\n",
    "                    gene_symbol = gene\n",
    "                    start = element[\"start\"]\n",
    "                    end = element[\"end\"]\n",
    "                    strand = element[\"strand\"]\n",
    "                    sequence_class = \"Unknown\"\n",
    "\n",
    "                    # Determine the sequence class\n",
    "                    for seq_class in seq_class_dicts[species]:\n",
    "                        if start >= int(seq_class[1]) and end <= int(seq_class[2]):\n",
    "                            sequence_class = seq_class[3]\n",
    "                            break\n",
    "\n",
    "                    # Print gene details to the file\n",
    "                    print(f\"{species}\\t{gene_name}\\t{gene_symbol}\\t{start}\\t{end}\\t{strand}\\t{sequence_class}\", file=gene_details_file)\n",
    "\n",
    "print(f\"Gene details have been saved to {gene_details_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin list of identities by values 50 to 55, 55 to 60, 60 to 65, 65 to 70, 70 to 75, 75 to 80, 80 to 85, 85 to 90, 90 to 95, 95 to 100\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.hist(list_of_all_identities, bins = 50, color = 'skyblue', edgecolor='black', alpha = 0.7)\n",
    "# plt.yscale('log')\n",
    "plt.xlabel('Identity', fontsize = 12)\n",
    "plt.ylabel('Count', fontsize = 12)\n",
    "plt.title('Histogram of identities', fontsize = 14)\n",
    "\n",
    "#remove frame and white space around plot\n",
    "sns.despine()\n",
    "# Add a black dashed vertical line at x = 97\n",
    "plt.axvline(x=97, color='black', linestyle='--')\n",
    "\n",
    "plt.savefig(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/identity_histogram.\")\n",
    "# plt.yscale('log')\n",
    "# plt.savefig(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/identity_histogram_log.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the ampliconic gene families \n",
    "# Defined as sequence identity for at least two copies have ≥97% sequence identity within at least one species.\n",
    "\n",
    "# Load the TSV file\n",
    "df = pd.read_csv(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/clusters_identities.tsv\", sep='\\t')\n",
    "\n",
    "# Function to check if at least 2 copies have >= 97% identity\n",
    "def is_ampliconic(sequence_identities):\n",
    "    # Split the string by semicolons and convert to floats\n",
    "    identities = [float(identity) for identity in sequence_identities.split(';') if identity]\n",
    "    \n",
    "    # Check if there are at least 2 identities >= 97%\n",
    "    return sum([identity >= 97.0 for identity in identities]) >= 1\n",
    "\n",
    "# Filter the rows based on the condition for each species (PanTro, HomSap)\n",
    "filtered_rows = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Check for each species column ('PanTro' and 'HomSap') \n",
    "    if is_ampliconic(row['PanTro']) or is_ampliconic(row['HomSap']) or is_ampliconic(row['PanPan']) or is_ampliconic(row['GorGor']) or is_ampliconic(row['PonPyg']) or is_ampliconic(row['PonAbe']) or is_ampliconic(row['SymSyn']) :\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "# Create a new dataframe with only the ampliconic families\n",
    "ampliconic_df = pd.DataFrame(filtered_rows)\n",
    "ampliconic_df\n",
    "\n",
    "# Save the filtered dataframe to a new TSV file\n",
    "ampliconic_df.to_csv(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/ampliconic_families.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the gene family to the genes of the gene_details.tsv file \n",
    "\n",
    "# Load the gene_details and clusters files\n",
    "gene_details_file = f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/gene_details.tsv\"\n",
    "clusters_file = f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/ampliconic_families.tsv\"\n",
    "\n",
    "gene_details_df = pd.read_csv(gene_details_file, sep='\\t')\n",
    "clusters_df = pd.read_csv(clusters_file, sep='\\t')\n",
    "\n",
    "# Initialize the new column in gene_details_df\n",
    "gene_details_df[\"gene_family_symbol\"] = \"\"\n",
    "\n",
    "# Iterate through each row in gene_details_df\n",
    "for index, row in gene_details_df.iterrows():\n",
    "    gene_name = row[\"Gene\"]\n",
    "    \n",
    "    # Search for the gene_name in the \"Description(s)\" column of clusters_df\n",
    "    matching_row = clusters_df[clusters_df[\"Description(s)\"].str.contains(gene_name, na=False, regex=False)]\n",
    "    \n",
    "    if not matching_row.empty:\n",
    "        # Get the gene_symbol from the matching row\n",
    "        gene_symbol = matching_row.iloc[0][\"gene_symbol\"]\n",
    "        # Append the gene_symbol to the \"gene_family_symbol\" column in gene_details_df\n",
    "        gene_details_df.at[index, \"gene_family_symbol\"] = gene_symbol\n",
    "\n",
    "# Save the updated DataFrame back to a new file\n",
    "gene_details_df.to_csv(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/gene_details_updated.tsv\", sep='\\t', index=False)\n",
    "\n",
    "print(\"Updated gene_details.tsv with gene_family_symbol column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply whether a gene lays within a palindrome\n",
    "gene_details_df = pd.read_csv(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/gene_details_updated.tsv\", sep='\\t')\n",
    "\n",
    "## Check if a gene is within any palindrome\n",
    "def check_palindrome(specie, gene_start, gene_end):\n",
    "    # Get the list of palindromes for the given species\n",
    "    if specie in palindrome_lines_per_species:\n",
    "        palindromes = palindrome_lines_per_species[specie]\n",
    "        \n",
    "        for palindrome in palindromes:\n",
    "            _, pal_start, pal_end, _, pal_name = palindrome\n",
    "            \n",
    "            # Convert coordinates to integers\n",
    "            pal_start = int(pal_start)\n",
    "            pal_end = int(pal_end)\n",
    "            \n",
    "            # Check if gene coordinates fall within the palindrome range\n",
    "            if gene_start >= pal_start and gene_end <= pal_end:\n",
    "                return 'yes', pal_name\n",
    "    \n",
    "    return 'no', None\n",
    "    \n",
    "# New columns to be added\n",
    "gene_details_df['in_palindrome'] = 'no'\n",
    "gene_details_df['palindrome_name'] = None\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for idx, row in gene_details_df.iterrows():\n",
    "    species = row['Species'] \n",
    "    gene_start = row['Start'] \n",
    "    gene_end = row['End']  \n",
    "    \n",
    "    in_palindrome, palindrome_name = check_palindrome(species, gene_start, gene_end)\n",
    "    \n",
    "    # Update the dataframe with the results\n",
    "    gene_details_df.at[idx, 'in_palindrome'] = in_palindrome\n",
    "    gene_details_df.at[idx, 'palindrome_name'] = palindrome_name\n",
    "\n",
    "# Save the updated dataframe to a new TSV file\n",
    "gene_details_df.to_csv(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/gene_details_updated_with_palindromes.tsv\", sep='\\t', index=False)\n",
    "\n",
    "gene_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE THE GENE COORDINATES!\n",
    "\n",
    "gene_details_file = f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/gene_details_updated_with_palindromes.tsv\"\n",
    "df = pd.read_csv(gene_details_file, sep=\"\\t\")\n",
    "df\n",
    "\n",
    "species_to_gff = {\n",
    "    d['species']: d['data']['path_to_annotation_NCBI'].replace(\".gff\", \"_chrY.gff\")\n",
    "    for d in data\n",
    "}\n",
    "\n",
    "def parse_gene_coords(gff_path):\n",
    "    coords = {}\n",
    "    with open(gff_path) as g:\n",
    "        for line in g:\n",
    "            if line.startswith('#'): continue\n",
    "            chrom,src,feat,start,end,_,_,_,attr = line.split('\\t',8)\n",
    "            if feat != 'gene': continue\n",
    "            # pull out gene_symbol (or fallback to gene=)\n",
    "            md = dict(item.split('=',1) for item in attr.split(';') if '=' in item)\n",
    "            sym = md.get('gene_symbol') or md.get('gene')\n",
    "            if sym:\n",
    "                coords[sym] = (int(start), int(end))\n",
    "    return coords\n",
    "\n",
    "gff_cache = {\n",
    "    sp: parse_gene_coords(path)\n",
    "    for sp, path in species_to_gff.items()\n",
    "}\n",
    "\n",
    "new_starts, new_ends = [], []\n",
    "for _, row in df.iterrows():\n",
    "    sp, sym = row['Species'], row['Gene_symbol']\n",
    "    start,end = gff_cache[sp].get(sym, (row['Start'], row['End']))\n",
    "    new_starts.append(start)\n",
    "    new_ends.append(end)\n",
    "\n",
    "df['Start'] = new_starts\n",
    "df['End']   = new_ends\n",
    "df\n",
    "\n",
    "# save the table \n",
    "df.to_csv(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/gene_details_updated_with_palindromes_coordinates.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows that have NaN in gene_family_symbol -> all the ones that are not part of a ampliconic gene family\n",
    "df = df.dropna(subset=[\"gene_family_symbol\"])\n",
    "df.to_csv(f\"{work_dir.replace('x_multicopy','y_multicopy')}/protein_extracted_longest/clusters_merged/gene_details_updated_with_palindromes_coordinates.tsv\", sep=\"\\t\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the sequences from the reference genome (done for all in a separate script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the X chromosome\n",
    "## make a gene coordinate .txt file for each species in the species list (defined all the way at the start)\n",
    "\n",
    "# Read the X chromosome file in again\n",
    "gene_details_df = pd.read_csv(f\"{work_dir}/protein_extracted_longest/clusters_merged/gene_details_updated_with_palindromes.tsv\", sep='\\t')\n",
    "\n",
    "# Drop the genes (rows) that are not part of ampliconic gene families -> so have an NaN in the gene_family_symbol column \n",
    "gene_details_df = gene_details_df.dropna(subset=['gene_family_symbol'])\n",
    "\n",
    "# Dictionary mapping species to their specific chromosome names. \n",
    "chrom_map = {\n",
    "    'PanTro': 'NC_072421.2',\n",
    "    'HomSap': 'NC_060947.1'\n",
    "}\n",
    "\n",
    "# Function to create genes.txt for each species\n",
    "for species in species_list:\n",
    "    # Filter for the specific species\n",
    "    species_df = gene_details_df[gene_details_df['Species'] == species]\n",
    "    \n",
    "    # Use the predefined work_dir and creeate the gene file\n",
    "    output_path = f\"{work_dir}/{species}_genes.txt\"\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        for _, row in species_df.iterrows():\n",
    "            f.write(f\"{row['Gene_symbol']} {chrom_map[species]} {row['Start']} {row['End']}\\n\")\n",
    "\n",
    "print(\"Genes.txt files created for each species.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the genes file per species. Now make a FASTA file where the specific sequences of the genes are extracted based on the coordinates of the genes. \n",
    "from pyfaidx import Fasta\n",
    "\n",
    "def extract_gene_sequences(data):\n",
    "    for species_info in data:\n",
    "        species = species_info['species']\n",
    "        genome_file = species_info['data']['ref']\n",
    "        genes_file = f\"{work_dir}/{species}_genes.txt\"\n",
    "        output_file = f\"{work_dir}/{species}_gene_sequences.fasta\"\n",
    "        \n",
    "        # Load genome\n",
    "        genome = Fasta(genome_file)\n",
    "        \n",
    "        # Open output file for writing\n",
    "        with open(output_file, 'w') as out_f:\n",
    "            # Read genes file\n",
    "            with open(genes_file, 'r') as gene_f:\n",
    "                for line in gene_f:\n",
    "                    gene, chrom, start, end = line.strip().split()\n",
    "                    start, end = int(start), int(end)\n",
    "                    \n",
    "                    # Extract sequence\n",
    "                    sequence = genome[chrom][start-1:end]\n",
    "                    \n",
    "                    # Write to output file\n",
    "                    out_f.write(f\">{gene}\\n{sequence}\\n\")\n",
    "        \n",
    "        print(f\"Extracted sequences for {species}\")\n",
    "\n",
    "# Call the function with your data list\n",
    "extract_gene_sequences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the X and Y one from reference chromosome\n",
    "# X chromsome PanTro\n",
    "cmd = f\"samtools faidx {data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/GCF_028858775.2_NHGRI_mPanTro3-v2.0_pri_genomic.fna NC_072421.2 > {data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/PanTro_X.fasta\"\n",
    "subprocess.run(cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the new fasta files\n",
    "cmd = f\"samtools faidx {data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/PanTro_X.fasta\"\n",
    "subprocess.run(cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each species in the data list\n",
    "for species_entry in data:\n",
    "    species = species_entry['species']  # Get species identifier\n",
    "    genome_data = species_entry['data']  # Access genome data dictionary\n",
    "    \n",
    "    ref_genome = genome_data['ref']  # Reference genome path\n",
    "    chr_x = genome_data['chr_x']  # X chromosome identifier\n",
    "    chr_y = genome_data.get('chr_y')  # Y chromosome identifier\n",
    "    \n",
    "    # Define output file paths\n",
    "    x_fasta = f\"{os.path.dirname(ref_genome)}/{species}_X.fasta\"\n",
    "    y_fasta = f\"{os.path.dirname(ref_genome)}/{species}_Y.fasta\"\n",
    "\n",
    "    # Extract X chromosome\n",
    "    cmd_x = f\"samtools faidx {ref_genome} {chr_x} > {x_fasta}\"\n",
    "    subprocess.run(cmd_x, shell=True, check=True)\n",
    "    \n",
    "    # Index the X chromosome FASTA file\n",
    "    subprocess.run(f\"samtools faidx {x_fasta}\", shell=True, check=True)\n",
    "\n",
    "    # Extract Y chromosome\n",
    "    cmd_y = f\"samtools faidx {ref_genome} {chr_y} > {y_fasta}\"\n",
    "    subprocess.run(cmd_y, shell=True, check=True)\n",
    "\n",
    "    # Index the Y chromosome FASTA file\n",
    "    subprocess.run(f\"samtools faidx {y_fasta}\", shell=True, check=True)\n",
    "\n",
    "    print(f\"✅ Processed {species}: X and Y chromosome FASTA files created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BLAST human VCY against macaque Y chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Blast.Applications import NcbiblastnCommandline, NcbitblastnCommandline\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "human_protein = f\"{data_dir}/human_vcy.faa\"  # Your human protein sequence\n",
    "macfas_y = f\"{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/MacFas_Y.fasta\"\n",
    "output = f\"{data_dir}/blast_results_macfas_vcy.txt\"\n",
    "\n",
    "# Run tBLASTn (protein query vs nucleotide database)\n",
    "tblastn_cline = NcbitblastnCommandline(\n",
    "    query=human_protein,\n",
    "    subject=macfas_y,\n",
    "    out=output,\n",
    "    outfmt=\"6 qseqid sseqid pident length qstart qend sstart send evalue bitscore\",\n",
    "    evalue=1e-5\n",
    ")\n",
    "\n",
    "stdout, stderr = tblastn_cline()\n",
    "\n",
    "# Read and display results\n",
    "df = pd.read_csv(output, sep='\\t', names=[\n",
    "    'query', 'subject', 'pident', 'length', 'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore'\n",
    "])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "human_protein = f\"{data_dir}/human_vcy.faa\"  # Your human protein sequence\n",
    "macfas_y = f\"{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/MacFas_Y.fasta\"\n",
    "output = f\"{data_dir}/blast_results_macfas_vcy.txt\"\n",
    "\n",
    "# Run tBLASTn\n",
    "cmd = [\n",
    "    \"tblastn\",\n",
    "    \"-query\", human_protein,\n",
    "    \"-subject\", macfas_y,\n",
    "    \"-out\", output,\n",
    "    \"-outfmt\", \"6 qseqid sseqid pident length qstart qend sstart send evalue bitscore\",\n",
    "    \"-evalue\", \"10\"  # More permissive\n",
    "]\n",
    "\n",
    "subprocess.run(cmd)\n",
    "\n",
    "# Read results\n",
    "try:\n",
    "    df = pd.read_csv(output, sep='\\t', names=[\n",
    "        'query', 'subject', 'pident', 'length', 'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore'\n",
    "    ])\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No hits found. Try:\")\n",
    "        print(\"1. Check your protein sequence file exists\")\n",
    "        print(\"2. Make evalue less stringent (try 1e-1 or 10)\")\n",
    "        print(\"3. The gene might be too divergent or absent\")\n",
    "    else:\n",
    "        print(df)\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"No hits found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if VCY is already annotated in macaque\n",
    "#macfas_prot = f\"{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/protein.faa\"\n",
    "human_prot = f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/protein.faa\",\n",
    "import subprocess\n",
    "\n",
    "# Search for \"testis-specific basic protein Y\"\n",
    "result = subprocess.run(\n",
    "    f\"grep '^>' {human_prot} | grep -i 'testis-specific basic protein Y'\", \n",
    "    shell=True, \n",
    "    capture_output=True, \n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "# Human Y chromosome coordinates for VCY\n",
    "human_ref = f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/GCF_009914755.1_T2T-CHM13v2.0_genomic.fna\"\n",
    "chr_y = \"NC_060948.1\"  # Human Y chromosome\n",
    "\n",
    "# Extract region (example coordinates - update with yours)\n",
    "start = 14892489  # Your VCY start\n",
    "end = 14892764    # Your VCY end\n",
    "\n",
    "# Extract sequence\n",
    "for record in SeqIO.parse(human_ref, \"fasta\"):\n",
    "    if chr_y in record.id:\n",
    "        vcy_seq = record.seq[start-1:end]  # -1 for 0-based indexing\n",
    "        with open(\"human_VCY_cds.fna\", \"w\") as out:\n",
    "            out.write(f\">Human_VCY\\n{vcy_seq}\\n\")\n",
    "        print(f\"Extracted {len(vcy_seq)} bp\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"blastn\",  # nucleotide vs nucleotide\n",
    "    \"-query\", \"human_VCY_cds.fna\",\n",
    "    \"-subject\", f\"{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/MacFas_Y.fasta\",\n",
    "    \"-out\", \"vcy_blast.txt\",\n",
    "    \"-outfmt\", \"6\",\n",
    "    \"-evalue\", \"1e-3\"\n",
    "]\n",
    "\n",
    "subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "# Paths\n",
    "human_ref = f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/GCF_009914755.1_T2T-CHM13v2.0_genomic.fna\"\n",
    "\n",
    "# Human Y chromosome\n",
    "chr_y = \"NC_060948.1\"  # Human Y chromosome\n",
    "\n",
    "# Your VCY exon 1 coordinates\n",
    "start = 14962884  # Replace with your actual start\n",
    "end = 14963615    # Replace with your actual end\n",
    "\n",
    "# Extract sequence\n",
    "for record in SeqIO.parse(human_ref, \"fasta\"):\n",
    "    if chr_y in record.id:\n",
    "        exon_seq = record.seq[start-1:end]  # -1 because Python is 0-based\n",
    "        \n",
    "        # Save to file\n",
    "        with open(\"human_VCY1B_gene.fasta\", \"w\") as out:\n",
    "            out.write(f\">Human_VCY_exon1_{start}_{end}\\n{exon_seq}\\n\")\n",
    "        \n",
    "        print(f\"Extracted {len(exon_seq)} bp from {chr_y}:{start}-{end}\")\n",
    "        print(f\"Saved to: human_VCY_exon1.fna\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
