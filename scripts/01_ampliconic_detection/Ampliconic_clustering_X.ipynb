{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735104dc-95a0-4f45-aeb4-716ca5da1833",
   "metadata": {},
   "source": [
    "# X chromosome ampliconic clustering updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5448d251-e9d4-4f32-abd2-0ee3cae0690a",
   "metadata": {},
   "source": [
    "Goal of script: Make ampliconic clusters based on nucleotide sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85cab59-b807-454f-981d-2881c83195a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import Phylo\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import re\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c988aff-6975-465b-9d18-61710684697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base directories\n",
    "data_dir = \"/home/emma/Amplicons/Workspaces/emma/downloaded_data\"\n",
    "work_dir = os.path.join(data_dir, \"work_dir\", \"x_multicopy\")\n",
    "\n",
    "#Define the list of dictionaries (data) containing genome information for different species\n",
    "data = [\n",
    "    {'species':'PanTro',\n",
    "     'data': {'chr_y': \"NC_072422.2\",\n",
    "              'chr_x': \"NC_072421.2\",\n",
    "              'path_to_annotation_NCBI': f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/genomic.gff\",\n",
    "              'path_to_annotation_NCBI_chry': f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/genomic_chrY.gff\", \n",
    "              'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/GCF_028858775.2.gff3\",\n",
    "              'ref':  f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/GCF_028858775.2_NHGRI_mPanTro3-v2.0_pri_genomic.fna\", \n",
    "              'rna':  f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/rna.fna\",\n",
    "              'prot': f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/protein.faa\", \n",
    "              'cds': f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/cds_from_genomic.fna\", \n",
    "              'gff_x': f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/genomic_chrX.gff\", \n",
    "              'fasta_x': f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/PanTro_X.fasta\", \n",
    "              'gff_x_cds': f\"{data_dir}/references/PanTro/ncbi_dataset/data/GCF_028858775.2/genomic_chrX_cds_isoform.gff\", \n",
    " }},\n",
    "    {'species':'HomSap',\n",
    "     'data': {'chr_y': \"NC_060948.1\",\n",
    "              'chr_x': \"NC_060947.1\",\n",
    "              'path_to_annotation_NCBI': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/genomic.gff\",\n",
    "              'path_to_annotation_NCBI_chry': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/genomic_chrY.gff\",\n",
    "              'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/hg38.gff3\",\n",
    "              'ref': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/GCF_009914755.1_T2T-CHM13v2.0_genomic.fna\",\n",
    "              'cds': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/cds_from_genomic.fna\",\n",
    "              'prot': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/protein.faa\",\n",
    "              'gff_x': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/genomic_chrX.gff\",\n",
    "              'fasta_x': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/HomSap_X.fasta\",\n",
    "              'gff_x_cds': f\"{data_dir}/references/HomSap/ncbi_dataset/data/GCF_009914755.1/genomic_chrX_cds_isoform.gff\",\n",
    "\n",
    "              }},\n",
    "     {'species':'PanPan',\n",
    "     'data': {'chr_y': \"NC_073273.2\",\n",
    "              'chr_x': \"NC_073272.2\",\n",
    "              'path_to_annotation_NCBI': f\"{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/genomic.gff\",\n",
    "              'path_to_annotation_NCBI_chry': f\"{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/genomic_chrY.gff\",\n",
    "              'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/GCF_029289425.2.gff3\",\n",
    "              'ref': f\"{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/GCF_029289425.2_NHGRI_mPanPan1-v2.0_pri_genomic.fna\",\n",
    "              'cds': f'{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/cds_from_genomic.fna',\n",
    "              'prot': f'{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/protein.faa',\n",
    "              'gff_x': f\"{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/genomic_chrX.gff\",\n",
    "              'fasta_x': f\"{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/PanPan_X.fasta\",\n",
    "              'gff_x_cds': f\"{data_dir}/references/PanPan/ncbi_dataset/data/GCF_029289425.2/genomic_chrX_cds_isoform.gff\",\n",
    "              }},\n",
    "      {'species':'GorGor',\n",
    "     'data': {'chr_y': \"NC_073248.2\",\n",
    "              'chr_x': \"NC_073247.2\",\n",
    "              'path_to_annotation_NCBI': f\"{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/genomic.gff\",\n",
    "              'path_to_annotation_NCBI_chry': f\"{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/genomic_chrY.gff\",\n",
    "              'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/GCF_029281585.2.gff3\",\n",
    "              'ref': f'{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/GCF_029281585.2_NHGRI_mGorGor1-v2.0_pri_genomic.fna',\n",
    "              'cds': f'{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/cds_from_genomic.fna',\n",
    "              'prot': f'{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/protein.faa',\n",
    "              'gff_x': f\"{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/genomic_chrX.gff\",\n",
    "              'fasta_x': f\"{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/GorGor_X.fasta\",\n",
    "                            'gff_x_cds': f\"{data_dir}/references/GorGor/ncbi_dataset/data/GCF_029281585.2/genomic_chrX_cds_isoform.gff\",\n",
    "              }},\n",
    "    {'species':'PonPyg',\n",
    "     'data': {'chr_y': \"NC_072397.2\",\n",
    "              'chr_x': \"NC_072396.2\",\n",
    "              'path_to_annotation_NCBI': f\"{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/genomic.gff\",\n",
    "              'path_to_annotation_NCBI_chry': f\"{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/genomic_chrY.gff\",\n",
    "              'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/GCF_028885625.2.gff3\",\n",
    "              'ref': f'{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/GCF_028885625.2_NHGRI_mPonPyg2-v2.0_pri_genomic.fna',\n",
    "              'cds': f'{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/cds_from_genomic.fna',\n",
    "              'prot': f'{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/protein.faa',\n",
    "              'gff_x': f\"{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/genomic_chrX.gff\",\n",
    "              'fasta_x': f\"{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/PonPyg_X.fasta\",\n",
    "              'gff_x_cds': f\"{data_dir}/references/PonPyg/ncbi_dataset/data/GCF_028885625.2/genomic_chrX_cds_isoform.gff\",\n",
    "              }},\n",
    "    {'species':'PonAbe',\n",
    "     'data': {'chr_y': \"NC_072009.2\",\n",
    "              'chr_x': \"NC_072008.2\",\n",
    "              'path_to_annotation_NCBI': f\"{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/genomic.gff\",\n",
    "              'path_to_annotation_NCBI_chry': f\"{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/genomic_chrY.gff\",\n",
    "              'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/GCF_028885655.2.gff3\",\n",
    "              'ref': f'{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/GCF_028885655.2_NHGRI_mPonAbe1-v2.0_pri_genomic.fna',\n",
    "              'cds': f'{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/cds_from_genomic.fna',\n",
    "              'prot': f'{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/protein.faa',\n",
    "              'gff_x': f\"{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/genomic_chrX.gff\",\n",
    "              'fasta_x': f\"{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/PonAbe_X.fasta\",\n",
    "              'gff_x_cds': f\"{data_dir}/references/PonAbe/ncbi_dataset/data/GCF_028885655.2/genomic_chrX_cds_isoform.gff\",\n",
    "              }},    \n",
    "    {'species':'SymSyn',\n",
    "      'data': {'chr_y': \"NC_072448.2\",\n",
    "               'chr_x': \"NC_072447.2\",\n",
    "               'ref': f'{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/GCF_028878055.3_NHGRI_mSymSyn1-v2.1_pri_genomic.fna',\n",
    "               'path_to_annotation_NCBI': f\"{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/genomic.gff\",\n",
    "               'path_to_annotation_NCBI_chry': f\"{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/genomic_chrY.gff\",\n",
    "               'path_to_annotation_CAT': f\"{data_dir}/CAT/consensus_gene_set/GCF_028878055.3.gff3\",\n",
    "               'cds': f'{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/cds_from_genomic.fna',\n",
    "               'prot': f'{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/protein.faa',\n",
    "               'gff_x': f\"{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/genomic_chrX.gff\",\n",
    "               'fasta_x': f\"{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/SymSyn_X.fasta\",\n",
    "               'gff_x_cds': f\"{data_dir}/references/SymSyn/ncbi_dataset/data/GCF_028878055.3/genomic_chrX_cds_isoform.gff\",\n",
    "               }},\n",
    "    {'species':'MacFas',\n",
    "      'data': {'chr_y': \"NC_132903.1\",\n",
    "               'chr_x': \"NC_088395.1\",\n",
    "               'ref': f'{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/GCF_037993035.2_T2T-MFA8v1.1_genomic.fna',\n",
    "               'path_to_annotation_NCBI': f\"{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/genomic.gff\",\n",
    "               'cds': f'{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/cds_from_genomic.fna',\n",
    "               'prot': f'{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/protein.faa',\n",
    "               'gff_x': f\"{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/genomic_chrX.gff\",\n",
    "               'fasta_x': f\"{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/MacFas_X.fasta\",\n",
    "               'gff_x_cds': f\"{data_dir}/references/MacFas/ncbi_dataset/data/GCF_037993035.2/genomic_chrX_cds_isoform.gff\",\n",
    "               }},\n",
    "]\n",
    "\n",
    "# Maps species identifiers to their common name\n",
    "species_to_sequence_spec = {\n",
    "    'PanTro': 'chimpanzee',\n",
    "    'HomSap': 'human',\n",
    "    'PanPan': 'bonobo',\n",
    "    'GorGor': 'gorilla',\n",
    "    'PonPyg': 'b-orang',\n",
    "    'PonAbe': 's-orang',\n",
    "    'SymSyn': 'siamang',\n",
    "    'MacFas': 'macaque'\n",
    "\n",
    "}\n",
    "# Extracting species names -> list of species identifiers by iterating t\n",
    "species_info = {item['species']: item['data'] for item in data}\n",
    "species_list = [d['species'] for d in data]\n",
    "species_info\n",
    "species_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fed44d-4481-4173-985a-32370a6d6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load dataframes with Gene coordinates and Family information\n",
    "\n",
    "# DataFrame with gene coordinates and other details\n",
    "genes = pd.read_csv(f\"{work_dir}/protein_extracted_longest/clusters_merged/gene_details_updated_with_palindromes_coordinates_x.tsv\", sep='\\t')\n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffb337-ba7d-473a-8522-105ee079da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals = genes['gene_family_symbol'].unique()\n",
    "print(unique_vals)\n",
    "len(unique_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a7645-e920-46d5-9454-bb7be4f05ff6",
   "metadata": {},
   "source": [
    "## Extract coding sequences per family "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4421c-1bfb-436c-a8fc-820241b7a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes: \n",
    "#'endogenous retrovirus group K member 6 Env polyprotein-like' is called endogenous\n",
    "#'putative uncharacterized protein FLJ39060' is called FLJ39060\n",
    "#'collagen alpha-4(IV) chain-like' is called collages\n",
    "# 'uncharacterized LOC129475109' is called LOC129475109\n",
    "# 'uncharacterized LOC115932372' is called LOC115932372\n",
    "# INTSL6 contains SAGE gene family "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453807e-2e6d-43d7-805e-34197a50e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which family to focus on -> this is called back later every time\n",
    "families = ['CSF2RA', 'SPANX', 'TBL1X' ,'VCX' ,'TMSB' ,'MAGEB',\n",
    " 'TCEAL8' ,'H2A','endogenous','SPACA5',\n",
    " 'SSX' ,'GAGE' ,'NUDT10' ,'CENPVL',\n",
    " 'FLJ39060' ,'XAGE1' ,'FAM156', 'SPIN',\n",
    " 'ZXD' ,'CXorf49' ,'DMRTC1' ,'FAM236', 'PABPC', 'RPL36A', 'ARMCX' ,'NXF',\n",
    " 'TCP11X2' ,'GPRASP', 'RAB40A' ,'H2BW', 'CT47' ,'RHOXF2' ,'SMIM10' ,'ETD',\n",
    " 'INTS6L', 'CT45A', 'CXorf51', 'EOLA' ,'HSFX' ,'TMEM185A', 'CSAG', 'PNMA',\n",
    " 'PWWP4', 'OPN1LW', 'TEX28', 'LAGE3', 'IKBKG' ,'F8A1',\n",
    " 'collagen' ,'LOC129475109','LOC115932372', 'MAGED1']\n",
    "len(families)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9727c577-7976-4487-919c-5c34aab2c14d",
   "metadata": {},
   "source": [
    "### Extract the coding Regions\n",
    "From each gene extract the CDS and merge all the species together in one large fasta file. <br>\n",
    "Translate the sequence at the end so that the sequences can be checked with the protein files in the references genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d448325-0cb2-4121-8cbc-aac9bccefc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the sequences\n",
    "import re\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "\n",
    "def parse_attributes(attr_str):\n",
    "    attrs = {}\n",
    "    for part in attr_str.strip().split(\";\"):\n",
    "        if \"=\" in part:\n",
    "            k, v = part.split(\"=\", 1)\n",
    "            attrs[k.strip().lower()] = v.strip()\n",
    "    return attrs\n",
    "\n",
    "def get_isoform(attrs):\n",
    "    \"\"\"Extract isoform identifier from attributes.\"\"\"\n",
    "    # Look for \"isoform X\" pattern in product field\n",
    "    prod = attrs.get(\"product\", \"\")\n",
    "    # More specific regex: capture X followed by digits, or standalone digit/letter\n",
    "    m = re.search(r'isoform\\s+(X\\d+|\\d+|[a-z])\\b', prod, re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).upper()  # normalize to uppercase\n",
    "    \n",
    "    # Fallback to transcript_id or protein_id\n",
    "    for key in (\"transcript_id\", \"protein_id\"):\n",
    "        if key in attrs:\n",
    "            return attrs[key]\n",
    "    \n",
    "    return \"NA\"\n",
    "\n",
    "def select_best_isoform(isoforms_dict, refseq_iso):\n",
    "    \"\"\"Select best isoform according to priority rules.\"\"\"\n",
    "    cands = list(isoforms_dict.keys())\n",
    "    \n",
    "    # Priority 1: RefSeq Select\n",
    "    if refseq_iso and refseq_iso in cands:\n",
    "        return refseq_iso\n",
    "    \n",
    "    # Priority 2: NA (unnamed isoform)\n",
    "    if \"NA\" in cands:\n",
    "        return \"NA\"\n",
    "    \n",
    "    # Priority 3: X1 (exact match only!)\n",
    "    if \"X1\" in cands:\n",
    "        return \"X1\"\n",
    "    \n",
    "    # Priority 4: 1 or A\n",
    "    for iso in (\"1\", \"A\"):\n",
    "        if iso in cands:\n",
    "            return iso\n",
    "    \n",
    "    # Priority 5: Lowest number or alphabetically first\n",
    "    # Separate X-prefixed numbers, plain numbers, and alphabetic\n",
    "    x_numeric = []\n",
    "    plain_numeric = []\n",
    "    alpha_isos = []\n",
    "    \n",
    "    for iso in cands:\n",
    "        # Match X followed by numbers (X4, X10, X123)\n",
    "        m_x = re.match(r'^X(\\d+)$', iso, re.IGNORECASE)\n",
    "        if m_x:\n",
    "            x_numeric.append((int(m_x.group(1)), iso))\n",
    "            continue\n",
    "        \n",
    "        # Match plain numbers (2, 3, 10)\n",
    "        m_plain = re.match(r'^(\\d+)$', iso)\n",
    "        if m_plain:\n",
    "            plain_numeric.append((int(m_plain.group(1)), iso))\n",
    "            continue\n",
    "        \n",
    "        # Everything else is alphabetic\n",
    "        alpha_isos.append(iso)\n",
    "    \n",
    "    # Prioritize X-numbers, then plain numbers, then alphabetic\n",
    "    if x_numeric:\n",
    "        x_numeric.sort()\n",
    "        return x_numeric[0][1]\n",
    "    \n",
    "    if plain_numeric:\n",
    "        plain_numeric.sort()\n",
    "        return plain_numeric[0][1]\n",
    "    \n",
    "    if alpha_isos:\n",
    "        alpha_isos.sort()\n",
    "        return alpha_isos[0]\n",
    "    \n",
    "    # Fallback: just return first alphabetically\n",
    "    return sorted(cands)[0]\n",
    "\n",
    "def extract_cds(genome_fasta, gff_file, gene_name):\n",
    "    \"\"\"Extract CDS for gene, selecting best isoform.\"\"\"\n",
    "    genome = SeqIO.to_dict(SeqIO.parse(genome_fasta, \"fasta\"))\n",
    "    records = {}      # isoform → list of CDS fragments\n",
    "    attrs_map = {}    # isoform → parsed attrs\n",
    "    refseq_iso = None\n",
    "    \n",
    "    # Collect all CDS features for this gene\n",
    "    with open(gff_file) as fh:\n",
    "        for line in fh:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            if len(cols) < 9 or cols[2] != \"CDS\":\n",
    "                continue\n",
    "            \n",
    "            seqid, _, _, start, end, _, strand, frame, attr_str = cols[:9]\n",
    "            attrs = parse_attributes(attr_str)\n",
    "            \n",
    "            if attrs.get(\"gene\") != gene_name:\n",
    "                continue\n",
    "            \n",
    "            iso = get_isoform(attrs)\n",
    "            \n",
    "            # Create a tuple representing this CDS fragment\n",
    "            # (seqid, start, end, strand) - these define a unique fragment\n",
    "            fragment = (seqid, int(start) - 1, int(end), int(frame), strand)\n",
    "            \n",
    "            # Only add if we haven't seen this exact fragment for this isoform\n",
    "            if iso not in records:\n",
    "                records[iso] = []\n",
    "            \n",
    "            # Check if this exact coordinate already exists for this isoform\n",
    "            fragment_coords = (fragment[0], fragment[1], fragment[2], fragment[4])  # seqid, start, end, strand\n",
    "            existing_coords = [(f[0], f[1], f[2], f[4]) for f in records[iso]]\n",
    "            \n",
    "            if fragment_coords not in existing_coords:\n",
    "                records[iso].append(fragment)\n",
    "            \n",
    "            # Store attributes (will be overwritten if duplicate, but that's fine)\n",
    "            attrs_map[iso] = attrs\n",
    "            \n",
    "            # Check for RefSeq Select tag\n",
    "            if \"tag\" in attrs and \"refseq select\" in attrs[\"tag\"].lower():\n",
    "                refseq_iso = iso\n",
    "    \n",
    "    if not records:\n",
    "        return {}\n",
    "    \n",
    "    # Assemble CDS sequence for each isoform\n",
    "    isoform_sequences = {}\n",
    "    for iso, frags in records.items():\n",
    "        rev = (frags[0][4] == \"-\")\n",
    "        frags.sort(key=lambda x: x[1], reverse=rev)\n",
    "        \n",
    "        pieces = []\n",
    "        first = True\n",
    "        for sid, s, e, frm, strand in frags:\n",
    "            subseq = genome[sid].seq[s:e]\n",
    "            if strand == \"-\":\n",
    "                subseq = subseq.reverse_complement()\n",
    "            if first:\n",
    "                subseq = subseq[frm:]  # Apply frame offset to first fragment\n",
    "                first = False\n",
    "            pieces.append(str(subseq))\n",
    "        \n",
    "        isoform_sequences[iso] = \"\".join(pieces)\n",
    "    \n",
    "    # Select best isoform\n",
    "    best_iso = select_best_isoform(isoform_sequences, refseq_iso)\n",
    "    \n",
    "    return {best_iso: (isoform_sequences[best_iso], attrs_map[best_iso])}\n",
    "\n",
    "# ========== Main processing loop ================\n",
    "for family in families:\n",
    "    print(f\"\\n=== Processing family {family!r} ===\")\n",
    "    filtered_genes = genes[\n",
    "        genes[\"gene_family_symbol\"].str.contains(family, na=False)\n",
    "    ]\n",
    "    \n",
    "    intermediate_dir = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform\"\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "    \n",
    "    for species in species_list:\n",
    "        info = species_info[species]\n",
    "        genome = info[\"fasta_x\"]\n",
    "        gff_file = info[\"gff_x_cds\"]\n",
    "        out_fa = f\"{intermediate_dir}/{species}_{family}.fa\"\n",
    "        \n",
    "        with open(out_fa, \"w\") as fout:\n",
    "            sp_genes = filtered_genes[filtered_genes[\"Species\"] == species]\n",
    "            written = 0\n",
    "            \n",
    "            for gene in sp_genes[\"Gene_symbol\"]:\n",
    "                iso_dict = extract_cds(genome, gff_file, gene)\n",
    "                \n",
    "                if not iso_dict:\n",
    "                    print(f\"    [!] No CDS for {gene} in {species}\")\n",
    "                    continue\n",
    "                \n",
    "                for iso, (seq, attrs) in iso_dict.items():\n",
    "                    pid = attrs.get(\"protein_id\", \"\")\n",
    "                    header = f\">{gene}_isoform_{iso}\"\n",
    "                    if pid:\n",
    "                        header += f\";protein_id={pid}\"\n",
    "                    fout.write(header + \"\\n\" + seq + \"\\n\")\n",
    "                    written += 1\n",
    "            \n",
    "            print(f\"  • {species}: wrote {written}/{len(sp_genes)} genes → {out_fa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193bef1d-5ae4-4351-86fe-0075eadb3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the fasta files\n",
    "for family in families:\n",
    "    print(f\"\\n→ Processing family {family!r}\\n\")\n",
    "\n",
    "    # … your per‐family + per‐species extraction code …\n",
    "\n",
    "    # combine step\n",
    "    intermediate_dir = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform\"\n",
    "    combined_fasta   = f\"{intermediate_dir}/all_species_{family}.fa\"\n",
    "    with open(combined_fasta, \"w\") as outfile:\n",
    "        for species in species_list:\n",
    "            species_fasta = f\"{intermediate_dir}/{species}_{family}.fa\"\n",
    "            try:\n",
    "                with open(species_fasta) as infile:\n",
    "                    for line in infile:\n",
    "                        if line.startswith(\">\"):\n",
    "                            header = line.strip()\n",
    "                            if not header.endswith(f\"_{species}\"):\n",
    "                                header += f\"_{species}\"\n",
    "                            outfile.write(header + \"\\n\")\n",
    "                        else:\n",
    "                            outfile.write(line)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"[Warning] {species_fasta} not found.\")\n",
    "    print(f\"→ Combined FASTA for {family} at {combined_fasta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b542c68-7372-48b8-abd2-3aedd9b3c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRANSLATE \n",
    "\n",
    "## Translate the coding sequence to check if there are stop codons within \n",
    "def translate_record(record, frame=0):\n",
    "    \"\"\"\n",
    "    Translates a SeqRecord's nucleotide sequence into a protein sequence.\n",
    "    \n",
    "    :param record: A SeqRecord object containing the nucleotide sequence.\n",
    "    :param frame: The reading frame (0, 1, or 2) to start translation.\n",
    "    :return: A new SeqRecord with the translated protein sequence.\n",
    "    \"\"\"\n",
    "    # Adjust the sequence for the reading frame\n",
    "    trimmed_seq = record.seq[frame:]\n",
    "    # Translate into protein (keeps '*' for stop codons)\n",
    "    protein_seq = trimmed_seq.translate(to_stop=False)\n",
    "    # Create a new SeqRecord for the protein\n",
    "    return SeqRecord(protein_seq, id=record.id, description=\"translated\")\n",
    "\n",
    "def translate_fasta(input_file, output_file, frame=0):\n",
    "    \"\"\"\n",
    "    Reads a FASTA file with nucleotide sequences, translates each sequence, \n",
    "    and writes the protein sequences to an output FASTA file.\n",
    "    \n",
    "    :param input_file: Path to the input FASTA file.\n",
    "    :param output_file: Path to the output FASTA file.\n",
    "    :param frame: The reading frame (0, 1, or 2) to start translation.\n",
    "    \"\"\"\n",
    "    # Parse the input FASTA file and translate each record\n",
    "    translated_records = []\n",
    "    for record in SeqIO.parse(input_file, \"fasta\"):\n",
    "        translated_records.append(translate_record(record, frame))\n",
    "    \n",
    "    # Write the translated records to the output FASTA file\n",
    "    SeqIO.write(translated_records, output_file, \"fasta\")\n",
    "\n",
    "for family in families:\n",
    "    # 1) make the per-family directory\n",
    "    intermediate_dir = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform\"\n",
    "    os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "    # 2) build your input/output paths\n",
    "    in_fasta  = f\"{intermediate_dir}/all_species_{family}.fa\"\n",
    "    out_fasta = f\"{intermediate_dir}/all_species_{family}_translated.fa\"\n",
    "\n",
    "    # 3) translate\n",
    "    print(f\"Translating {in_fasta} → {out_fasta}\")\n",
    "    translate_fasta(in_fasta, out_fasta, frame=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a77913-5f0d-49a2-8b76-87ef4c80f3aa",
   "metadata": {},
   "source": [
    "## Create Ampliconic clustering across species using BLAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8624ca3-cfde-4b91-bb12-2116b90241ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create ampliconic clusters based on Identity using BLASTN\n",
    "\n",
    "# use the makeblastbd command to create a BLAST database from the complete FASTA file of coding sequences sequences. \n",
    "# makeblastbd is a tool to create a database from sequence data. Prepares the input FASTA file for fast querying using BLAST. \n",
    "# - bdtype nucl -> defines the type of sequence in the database. nucl indicates that the sequence are nucleotide (prot would be for protein).\n",
    "#creates automatically directory to store the output per gene family\n",
    "\n",
    "for family in families:\n",
    "    print(f\"▶ makeblastdb for {family!r}\")\n",
    "    cmd = f\"makeblastdb \\\n",
    "            -in {data_dir}/sequences_x_updated/{family}_selected_isoform/all_species_{family}.fa \\\n",
    "            -dbtype nucl \\\n",
    "            -out {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/blastdb\"\n",
    "    subprocess.run(cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2122e25-998c-4ce6-b36c-247a3905f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then do a pairwise comparison of each gene coding sequence against all other genes in the database \n",
    "# The output (.blastp.tsv) provides detailed alignments and similarities for each nucleotide query against the database.\n",
    "\n",
    "#Run a BLAST search for each species' coding sequences against the BLAST database created. \n",
    "# blast all against DB\n",
    "     # blastn -> a tool for comparing nucleotide sequences (query) against the above made database\n",
    "     # -query -> specified the INPUT query file. This is the FASTA file of all the genes of all the species (same file as above)\n",
    "     # -db -> specified the BLAST DATABASE to search against (this is the one that is created by makeblastdb)\n",
    "     # -out -> specified the OUTPUT file for the resutls of the BLAST search. the results will be saved in a tab-seperated file (.blastp.tsv)\n",
    "     # -outfmt -> specified the output format for BLAST results. 6 means tabular format with custom columns.\n",
    "    # Selected columns then: qseqid (Query sequence ID, sseqid (subject (database) sequence ID), pident (percentage of identical matches), mismatch (number of mismatches), gapopen (number of gap openings)\n",
    "    # gaps (total number of gaps), qcovs (query coverage per subject sequence), qcovshsp (Query coverage per HSP (high scoring pair), evalue (Expected value, which measures the significance of the match).\n",
    "\n",
    "for family in families:\n",
    "    print(f\"Processing family: {family}\")\n",
    "    cmd = f\"blastn -query {data_dir}/sequences_x_updated/{family}_selected_isoform/all_species_{family}.fa \\\n",
    "    -db {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/blastdb \\\n",
    "    -out {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/results.blastp.tsv \\\n",
    "    -outfmt \\\"6 qseqid sseqid pident mismatch gapopen gaps qcovs qcovshsp evalue\\\" \"\n",
    "\n",
    "    subprocess.run(cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa070b-a32a-4a02-8895-203ee58e0d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the BLAST results to build a network of relationships (edges) between proteins based on similarity thresholds for identity, coverage, and score\n",
    "\n",
    "    # edges -> a dictionary to store relationships between sequences. \n",
    "    # Keys = query protein IDs and values=lists of subject (matched) protein IDs The thresholds are: \"identity (minimum percentage of identical matches (95%), coverage (miniumum query coverage (80%)), score -> maximum e-value (0.001), indicating the significance of the alignment)\n",
    "\n",
    "    #Takes the just made .blasp.tsv. and open the BLAST result file in read mode (with.open etc) for line in infile = Then process each line of the BLAST result file.\n",
    "    #line = line.strip().split(\"\\t\") -> Splits the line into a list of fields using tab (\\t) as the delimiter.\n",
    "\n",
    "    # if float(line[2]) >= identity and int(line[6]) >= coverage and float(line[7]) < score: -> applies the filtering criteria.\n",
    "    # identity -> Checks if the percentage identity (pident) is at least 95%.\n",
    "    #int(line[6]) >= coverage -> Checks if the query coverage (qcovs) is at least 80%.\n",
    "    # float(line[7]) < score -> Ensures the e-value (evalue) is below the threshold of 0.001\n",
    "\n",
    "    # if line[0] in edges: -> store the edges. If the query sequence (line[0]) is already in edges, it appends the subject sequence (line[1]) to its list of edges.\n",
    "    # If the query sequence is not yet in edges Create a new entry with the query protein as the key and a list containing the subject protein as the value\n",
    "\n",
    "    # The edges Dictionary: Represents a network of protein relationships for each species based on BLAST similarity. -> according to the filtering criteria it will so that protein 1 is similar to protein 3 and 5 for example.\n",
    "\n",
    "# # Collect edges for each protein with species-specific thresholds\n",
    "edges = {}\n",
    "identity_default = 95\n",
    "identity_macfas = 90  # More lenient threshold for MacFas because it is already 5% sequence identity away from the other species\n",
    "coverage = 80\n",
    "score = 0.001\n",
    "\n",
    "edges_per_family = {}\n",
    "\n",
    "for family in families:\n",
    "    print(f\"Processing family: {family}\")\n",
    "    edges = {}\n",
    "    file = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/results.blastp.tsv\"\n",
    "    \n",
    "    with open(file, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            cols = line.strip().split(\"\\t\")\n",
    "            \n",
    "            # Determine which identity threshold to use\n",
    "            # If either the query or subject is from MacFas, use the lower threshold\n",
    "            query_id = cols[0]\n",
    "            subject_id = cols[1]\n",
    "            \n",
    "            # Check if either sequence is from MacFas (adjust the pattern to match your naming convention)\n",
    "            is_macfas_comparison = \"MacFas\" in query_id or \"MacFas\" in subject_id\n",
    "            \n",
    "            # Use appropriate identity threshold\n",
    "            identity_threshold = identity_macfas if is_macfas_comparison else identity_default\n",
    "            \n",
    "            # Apply filtering with the appropriate threshold\n",
    "            if (float(cols[2]) >= identity_threshold\n",
    "                and int(cols[6]) >= coverage\n",
    "                and float(cols[7]) < score):\n",
    "                \n",
    "                if cols[0] in edges:\n",
    "                    edges[cols[0]].append(cols[1])\n",
    "                else:\n",
    "                    edges[cols[0]] = [cols[1]]\n",
    "    \n",
    "    edges_per_family[family] = edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a942ad0-82d5-495d-a0b2-af73409bf88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7587555-7cba-4faf-9920-ffc1107d6b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then refine the edges collected above, by keeping only two-way edges -> so both nodes must point to each other. It constructs a new list of two-way edges and updates the set of vertices accordingly.\n",
    "\n",
    "    # edges_2way = [] -> an empty list to store the two-way edges vertices = set() -> a set to store all unique sequences (nodes) that are part of these two-way edge\n",
    "    # for node_A in edges: -> iterates through each source node (node_A) in the edges dictionary\n",
    "    # for node_B in edges[node_A]: -> ensures that node_B exists as a source node in edges. if node_B in edges and node_A in edges[node_B]: -> hecks if the relationship is reciprocated (i.e., node_A is in the list of edges for node_B). -> if both are met, the edge between node_A and node_B is 2-way.\n",
    "    # tuple = (node_A, node_B) -> create a tuple representing the edge between node_A and node_B tuple = sorted(tuple) -> sorts the tuple to ensure consistent ordering (smaller node, larger node). This avoids duplicates.\n",
    "    # if tuple in edges_2way: continue -> If the tuple is already in edges_2way, it skips to the next iteration to avoid duplicates.\n",
    "    # edges_2way.append(tuple) -> adds the normalized edge tuple to edges_2way\n",
    "    # vertices.add(node_A) vertices.add(node_B) -> adds both nodes (node_A and node_B) to the vertices set to track all nodes involved in two-way edges\n",
    "    # Output -> A list of all 2way edges in edges_2way. Vertices is a set of all unique nodes (sequences) involved in two-way edges.\n",
    "\n",
    "two_way_per_family = {}\n",
    "vertices_per_family = {}\n",
    "\n",
    "for family in families:\n",
    "    print(f\"Processing family: {family}\")\n",
    "    edges     = edges_per_family[family]\n",
    "    edges_2way = []\n",
    "    vertices   = set()\n",
    "\n",
    "    for node_A in edges:\n",
    "        for node_B in edges[node_A]:\n",
    "            if node_B in edges and node_A in edges[node_B]:\n",
    "                pair = tuple(sorted((node_A, node_B)))\n",
    "                if pair in edges_2way:\n",
    "                    continue\n",
    "                edges_2way.append(pair)\n",
    "                vertices.add(node_A)\n",
    "                vertices.add(node_B)\n",
    "\n",
    "    two_way_per_family[family] = edges_2way\n",
    "    vertices_per_family[family]   = vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f77e4-820d-441d-8ed7-c3ed01a76676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge clusers of sequences using transitive clustering based on the two-way edges. The process involves 2 main steps: creating initial clusters and then merging clusters that share common elements.\n",
    "\n",
    "    # In first part. You intialize clusters -> a list of sets, where each set represents a cluster of connected nodes (sequences). You then iterate over the edges -> each edge in edges_2way represents a connection between two sequences (edge[0] and edge[1]).\n",
    "    # Then you check if Edge belongs to an exister cluster. For each cluster, check if theiter protein in the edge( edge[0] and edge[1]) is already part of a cluster. If yes -> both proteins are added to the cluster. The found=True indicate the edge has been incorporated. then you break out of the loop to avoid the redundant checks.\n",
    "    # Then you create a new cluster for disconnected edges -> if the edge does not belong to any existing cluster (found=False), create a new cluster containing the two sequences (set(edge)) and add it to clusters.\n",
    "\n",
    "    # The second part is to merge clusters with common elements. Initialize merged_cluster -> a lists of sets where overlapping clusters are mered into single unified clusters. Then you iterate over the initial cluster -> each cluster in clusters is checked for operlap with clusters in merged_clusters. Then you use cluster.intersection(merged_cluster) to check if there are any common elements between current cluster and any merged_cluster. \n",
    "    # If there is overlap (len(...) > 0), merge 2 clusters by updating merged_cluster with elements from cluster (merged_cluster.update(cluster). Mark found=True to indicate the cluster has been merged. Then break out of the loop to avoid redundent checks\n",
    "    # if no overlap is found -> add the cluster as a new indepedent entry in merged_clusters\n",
    "\n",
    "    # Output -> a list of sets, where each set represents a final cluster of connected nodes. overlapping clusters have been unified. This provides a higher-level organization of sequences into distinct clusters, which can then be used for downstream analysis, such as functional annotation or evolutionary relationships.\n",
    "\n",
    "clusters_per_family = {}\n",
    "merged_clusters_per_family = {}\n",
    "\n",
    "for family in families:\n",
    "    print(f\"→ Clustering for family {family!r}\")\n",
    "    edges_2way = two_way_per_family[family]\n",
    "\n",
    "    # 1) transitive clustering\n",
    "    clusters = []\n",
    "    for edge in edges_2way:\n",
    "        found = False\n",
    "        for cluster in clusters:\n",
    "            if edge[0] in cluster or edge[1] in cluster:\n",
    "                cluster.add(edge[0])\n",
    "                cluster.add(edge[1])\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            clusters.append(set(edge))\n",
    "\n",
    "    # 2) merge overlapping clusters\n",
    "    merged_clusters = []\n",
    "    for cluster in clusters:\n",
    "        found = False\n",
    "        for m in merged_clusters:\n",
    "            if cluster & m:       # any intersection?\n",
    "                m.update(cluster)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            merged_clusters.append(cluster)\n",
    "\n",
    "    # store and/or print\n",
    "    merged_clusters_per_family[family] = merged_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e0445-f02f-418f-85c4-604c9ab2b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_clusters_per_family['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc1cf1-35b2-40be-b6c9-0111b1cb7c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which sequences belong into cluster together\n",
    "# take the list of clusters, extract gene name, and species and create a dataframe with a cluster number for each gene. \n",
    "# build a list of (gene, species, cluster_id) tuples\n",
    "for family in families:\n",
    "    clusters = merged_clusters_per_family.get(family, [])\n",
    "    if not clusters:\n",
    "        print(f\"No clusters for family {family!r}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Build your list of (gene_name, species, cluster_id)\n",
    "    rows = [\n",
    "        (\n",
    "            gene,\n",
    "            gene.rsplit('_', 1)[1],      # species is the suffix after the last underscore\n",
    "            cluster_id\n",
    "        )\n",
    "        for cluster_id, cluster in enumerate(clusters, start=1)\n",
    "        for gene in cluster\n",
    "    ]\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(rows, columns=['gene_name', 'species', 'cluster'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    out_csv = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/gene_clustering_{family}.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"  • Saved clustering table for {family!r} → {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e383d-76e0-41cb-be51-1c0799b6f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives cluster names automatically based on overal recognizable cluster name \n",
    "for family in families:\n",
    "    print(f\"→ Naming clusters for family {family!r}\")\n",
    "\n",
    "    # 1) Load the per-family clustering table\n",
    "    csv_in = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/gene_clustering_{family}.csv\"\n",
    "    df = pd.read_csv(csv_in)\n",
    "\n",
    "    # 2) Auto-name each cluster\n",
    "    cluster_name = {}\n",
    "    species_combo_counts = {}\n",
    "\n",
    "    for cid, grp in df.groupby('cluster'):\n",
    "        # prefer non-LOC genes\n",
    "        non_loc = grp[~grp['gene_name'].str.startswith('LOC')]\n",
    "        if not non_loc.empty:\n",
    "            # take the shortest prefix before first '_'\n",
    "            prefs = non_loc['gene_name'].str.partition('_')[0]\n",
    "            base = prefs.loc[prefs.str.len().idxmin()]\n",
    "        else:\n",
    "            # fallback: name by species combination\n",
    "            combo = \"_\".join(sorted(grp['species'].unique()))\n",
    "            cnt = species_combo_counts.get(combo, 0) + 1\n",
    "            species_combo_counts[combo] = cnt\n",
    "            base = f\"{combo}_cluster{cnt}\"\n",
    "\n",
    "        cluster_name[cid] = base\n",
    "\n",
    "    # 3) Disambiguate duplicates so big clusters keep the base name\n",
    "    sizes = df.groupby('cluster').size().to_dict()\n",
    "    dups = defaultdict(list)\n",
    "    for cid, name in cluster_name.items():\n",
    "        dups[name].append(cid)\n",
    "\n",
    "    for name, cids in dups.items():\n",
    "        if len(cids) > 1:\n",
    "            # largest cluster keeps the base name\n",
    "            cids.sort(key=lambda x: sizes[x], reverse=True)\n",
    "            for idx, cid in enumerate(cids[1:], start=2):\n",
    "                cluster_name[cid] = f\"{name}_{idx}\"\n",
    "\n",
    "    # 4) Map back and save\n",
    "    df['cluster_name'] = df['cluster'].map(cluster_name)\n",
    "    out_csv = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/{family}_clusters_named_auto.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"  • Saved auto-named clusters → {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e31494-f1b8-4097-aa89-dd59edb6a54b",
   "metadata": {},
   "source": [
    "## Show the cluster counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8affeb-c5b9-4837-867b-eed293dd6a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show for each species (rows) and each cluster (columns), how many genes from that species are present in that cluster\n",
    "# group the gene-level dataframe and then bivor the results so that species are rows and cluster are the columns. \n",
    "\n",
    "# Assuming final_df is your DataFrame with columns: gene_name, species, cluster, ... (made above)\n",
    "# For example:\n",
    "#   gene_name          species  cluster\n",
    "# 0 PAGE4_GorGor      GorGor   1\n",
    "# 1 PAGE4_HomSap      HomSap   1\n",
    "# 2 XAGE2_GorGor      GorGor   2\n",
    "# ...\n",
    "\n",
    "for family in families:\n",
    "    print(f\"\\n→ Building species×cluster matrix for {family!r}\")\n",
    "\n",
    "    # 1) load the named‐cluster table\n",
    "    in_csv = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/{family}_clusters_named_auto.csv\"\n",
    "    df = pd.read_csv(in_csv)\n",
    "\n",
    "    # 2) count genes per (species, cluster)\n",
    "    grouped = (df.groupby(['species', 'cluster_name']).size().reset_index(name='count'))\n",
    "\n",
    "    # 3) pivot so species are rows, clusters are columns\n",
    "    cluster_species_df = (grouped.pivot(index='species', columns='cluster_name', values='count').fillna(0).sort_index().sort_index(axis=1))\n",
    "\n",
    "    # 4) show or save\n",
    "    #print(cluster_species_df)  # or display(...) in Jupyter\n",
    "    out_csv = (f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/species_cluster_counts_{family}.csv\")\n",
    "    cluster_species_df.to_csv(out_csv)\n",
    "    print(f\"  • Saved species×cluster counts → {out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06084e1e-4cee-4c7c-9747-8b5ff2d3c2d7",
   "metadata": {},
   "source": [
    "## Subset the fasta file of a gene family into its clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375ec81-02bb-4281-b92f-38c5d77a50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for family in families:\n",
    "    print(f\"\\n→ Processing family {family!r}\")\n",
    "\n",
    "    # 1) Load the auto‐named clusters table\n",
    "    csv_in = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/{family}_clusters_named_auto.csv\"\n",
    "    df = pd.read_csv(csv_in)\n",
    "\n",
    "    # 2) Build the cluster→genes map\n",
    "    cluster_to_genes = df.groupby(\"cluster_name\")[\"gene_name\"].apply(list).to_dict()\n",
    "\n",
    "    # (Optional) summary\n",
    "    for cluster, genes in cluster_to_genes.items():\n",
    "        print(f\"  Cluster: {cluster}, Number of genes: {len(genes)}\")\n",
    "\n",
    "    # 3) Load the full family FASTA once\n",
    "    fasta_in = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/all_species_{family}.fa\"\n",
    "    records = list(SeqIO.parse(fasta_in, \"fasta\"))\n",
    "    record_dict = {rec.id: rec for rec in records}\n",
    "    print(f\"  Loaded {len(record_dict)} FASTA records\")\n",
    "\n",
    "    # 4) Make output dir for per‐cluster FASTAs\n",
    "    cluster_dir = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_fastas\"\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "    # 5) Write one FASTA per cluster\n",
    "    for cluster, gene_list in cluster_to_genes.items():\n",
    "        selected = [record_dict[g] for g in gene_list if g in record_dict]\n",
    "        out_fa = f\"{cluster_dir}/{family}_cluster_{cluster}.fa\"\n",
    "        SeqIO.write(selected, out_fa, \"fasta\")\n",
    "        print(f\"    • Wrote {len(selected)} records → {out_fa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbd0f6-a98d-4e9f-9b28-2af88c61c8a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94cd63c1-d929-4c23-b9d2-ff7793e9c993",
   "metadata": {},
   "source": [
    "##  Make sure the correct gene names are in the coordinate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e827e-0440-4b36-a0e8-00c317c4102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Read once and filter out everything with no gene_family_symbol\n",
    "gene_details_file = f\"{work_dir}/protein_extracted_longest/clusters_merged/gene_details_updated_with_palindromes_coordinates.tsv\"\n",
    "\n",
    "master_df = pd.read_csv(gene_details_file, sep=\"\\t\")\n",
    "master_df = master_df.dropna(subset=[\"gene_family_symbol\"])\n",
    "\n",
    "# 2) Loop over each family\n",
    "for family in families:\n",
    "    print(f\"\\n→ Updating gene details for family {family!r}\")\n",
    "\n",
    "    # Load that family's auto-named clusters\n",
    "    cluster_file = os.path.join(\n",
    "        data_dir,\n",
    "        \"sequences_x_updated\",\n",
    "        f\"{family}_selected_isoform\",\n",
    "        \"blastdb\",\n",
    "        f\"{family}_clusters_named_auto.csv\"\n",
    "    )\n",
    "    cluster_df = pd.read_csv(cluster_file, sep=\",\")\n",
    "\n",
    "    # Subset the master table to only genes in this family\n",
    "    filt = master_df[\n",
    "        master_df['gene_family_symbol'].str.contains(family, na=False)\n",
    "    ]\n",
    "\n",
    "    # Prepare the merge keys\n",
    "    cluster_df['gene_prefix'] = cluster_df['gene_name'].str.split('_').str[0]\n",
    "    mapping = cluster_df[['gene_prefix', 'species', 'cluster_name']]\n",
    "\n",
    "    # Merge on Gene_symbol + Species → gene_prefix + species\n",
    "    merged = filt.merge(\n",
    "        mapping,\n",
    "        left_on=['Gene_symbol', 'Species'],\n",
    "        right_on=['gene_prefix', 'species'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Rename and drop helper columns\n",
    "    merged.rename(columns={'cluster_name': 'cluster'}, inplace=True)\n",
    "    drop_cols = [c for c in ('gene_prefix', 'species_y') if c in merged.columns]\n",
    "    if drop_cols:\n",
    "        merged.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    # (Optional) quick peek\n",
    "    print(f\"  {len(merged)} rows after merge (with new ‘cluster’ col)\")\n",
    "\n",
    "    # Write out the updated table\n",
    "    out_tsv = os.path.join(\n",
    "        data_dir,\n",
    "        \"sequences_x_updated\",\n",
    "        f\"{family}_selected_isoform\",\n",
    "        \"blastdb\",\n",
    "        f\"{family}_compl_gene_details_updated_with_palindromes_coordinates.tsv\"\n",
    "    )\n",
    "    merged.to_csv(out_tsv, sep=\"\\t\", index=False)\n",
    "    print(f\"  • Saved → {out_tsv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d033ea4d-9620-4c0d-bad2-5ca28cbe0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## one big file with all the information\n",
    "\n",
    "# 1) Load & pre-filter your master gene_details\n",
    "gene_details_file = f\"{work_dir}/protein_extracted_longest/clusters_merged/gene_details_updated_with_palindromes_coordinates.tsv\"\n",
    "\n",
    "master_df = pd.read_csv(gene_details_file, sep=\"\\t\")\n",
    "master_df = master_df.dropna(subset=[\"gene_family_symbol\"])\n",
    "\n",
    "\n",
    "# 2) Collect all per‐family cluster maps into one DataFrame\n",
    "maps = []\n",
    "for family in families:\n",
    "    fn = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/{family}_clusters_named_auto.csv\"\n",
    "    if not os.path.exists(fn):\n",
    "        print(f\"[!] missing cluster file for {family}, skipping\")\n",
    "        continue\n",
    "\n",
    "    cf = pd.read_csv(fn)\n",
    "    # extract the gene prefix (pre‐underscore) → Gene_symbol\n",
    "    cf[\"gene_prefix\"] = cf[\"gene_name\"].str.split(\"_\").str[0]\n",
    "    # keep only the columns we need\n",
    "    maps.append(cf[[\"gene_prefix\", \"species\", \"cluster_name\"]].rename(\n",
    "        columns={\"species\": \"Species\", \"cluster_name\": \"cluster\"}\n",
    "    ))\n",
    "\n",
    "# one big mapping table\n",
    "map_df = pd.concat(maps, ignore_index=True).drop_duplicates(\n",
    "    subset=[\"gene_prefix\", \"Species\"]\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Merge once onto master_df\n",
    "#    left_on Gene_symbol + Species  → right_on gene_prefix + Species\n",
    "merged_all = master_df.merge(map_df,\n",
    "    left_on=[\"Gene_symbol\", \"Species\"],\n",
    "    right_on=[\"gene_prefix\", \"Species\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# clean up helper columns\n",
    "if \"gene_prefix\" in merged_all.columns:\n",
    "    merged_all.drop(columns=[\"gene_prefix\"], inplace=True)\n",
    "\n",
    "# 4) Save your giant table\n",
    "out_file = f\"{data_dir}/sequences_x_updated/all_families_gene_details_with_clusters2.tsv\"\n",
    "\n",
    "merged_all.to_csv(out_file, sep=\"\\t\", index=False)\n",
    "print(f\"→ Wrote combined table with cluster info for all families → {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726fa19-d55c-4c8f-bbe3-8721c3c5144c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40fc278a-bd72-450d-9aff-4d6897868be8",
   "metadata": {},
   "source": [
    "## MEGA analysis dN & dS calculation per cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f6745-a866-4feb-8811-219b481a2e85",
   "metadata": {},
   "source": [
    "### Define all the clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5b9f3-0908-4b6e-b3f9-8c578e707507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the clusters\n",
    "import os\n",
    "\n",
    "# this dict will hold, for each family, the list of multi-seq clusters\n",
    "cluster_list_per_family = {}\n",
    "\n",
    "for family in families:\n",
    "    # ensure the alignments directory exists\n",
    "    cluster_alignments = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments\"\n",
    "    os.makedirs(cluster_alignments, exist_ok=True)\n",
    "\n",
    "    # grab every .fa basename in the cluster_fastas dir\n",
    "    cluster_dir = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_fastas\"\n",
    "    all_clusters = [\n",
    "        os.path.splitext(fn)[0]\n",
    "        for fn in os.listdir(cluster_dir)\n",
    "        if fn.endswith(\".fa\")\n",
    "    ]\n",
    "\n",
    "    # filter out FASTAs with only one sequence\n",
    "    filtered = []\n",
    "    for name in all_clusters:\n",
    "        path = os.path.join(cluster_dir, f\"{name}.fa\")\n",
    "        with open(path) as f:\n",
    "            nseq = sum(1 for line in f if line.startswith(\">\"))\n",
    "        if nseq > 1:\n",
    "            filtered.append(name)\n",
    "\n",
    "    # optional sanity‐check for duplicate IDs\n",
    "    for name in filtered:\n",
    "        path = os.path.join(cluster_dir, f\"{name}.fa\")\n",
    "        seen, dups = set(), set()\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\">\"):\n",
    "                    seqid = line[1:].split()[0]\n",
    "                    if seqid in seen:\n",
    "                        dups.add(seqid)\n",
    "                    else:\n",
    "                        seen.add(seqid)\n",
    "        if dups:\n",
    "            print(f\"[{family}] {name}.fa has duplicate IDs: {', '.join(dups)}\")\n",
    "\n",
    "    # store the filtered list for later\n",
    "    cluster_list_per_family[family] = filtered\n",
    "\n",
    "    print(f\"{family}: keeping {len(filtered)} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc2730-cf2c-4e2d-93f1-0900e50de9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6166fcd6-e4d1-4920-a1ef-479c6be38e81",
   "metadata": {},
   "source": [
    "### Make a Codon based Multi-Sequence Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b78ed5d-a4c4-47c5-9ca6-141dd530feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make a codon-based alignment\n",
    "# STEP 1: Align with MACSE\n",
    "\n",
    "for family in families:\n",
    "    # 1) ensure the output directory exists\n",
    "    ds_dir = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments\"\n",
    "    os.makedirs(ds_dir, exist_ok=True)\n",
    "    \n",
    "for family, cluster_list in cluster_list_per_family.items():\n",
    "    print(f\"\\n→ Aligning {len(cluster_list)} clusters for family {family!r}\")\n",
    "    cluster_alignments = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments\"\n",
    "\n",
    "    for cluster in cluster_list:\n",
    "        cmd = (\n",
    "            f\"macse -prog alignSequences \"\n",
    "            f\"-seq {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_fastas/{cluster}.fa \"\n",
    "            f\"-out_NT {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments/{cluster}_NT.fa \"\n",
    "            f\"-out_AA {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments/{cluster}_AA.fa\"\n",
    "        )\n",
    "        subprocess.run(cmd, shell=True, check=True,stdout=subprocess.DEVNULL,stderr=subprocess.DEVNULL)\n",
    "\n",
    "    print(f\"→ Done family {family!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb502220-f599-4759-893f-eae6126e42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## refine alignment in MACSE \n",
    "## to make the alignment better \n",
    "# run secondly+ seperately!! takes very long to run it at the same time\n",
    "\n",
    "for family, cluster_list in cluster_list_per_family.items():\n",
    "    print(f\"\\n→ Aligning {len(cluster_list)} clusters for family {family!r}\")\n",
    "    cluster_alignments = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments\"\n",
    "\n",
    "    # # Refine made alignment: for alignments that are difficult\n",
    "    for cluster in cluster_list:\n",
    "         cmd = (\n",
    "             f\"macse -prog refineAlignment \"\n",
    "             f\"-align {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments/{cluster}_NT.fa \"\n",
    "             f\"-out_NT {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments/{cluster}_NT.fa \"\n",
    "             f\"-out_AA {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments/{cluster}_AA.fa\"\n",
    "         )\n",
    "         subprocess.run(cmd, shell=True, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) # run quietly \n",
    "    \n",
    "    print(f\"→ Done family {family!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86197499-f0e3-465d-a71f-70fd7863fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean alignment in MACSE \n",
    "## to make the alignment useable for after \n",
    "\n",
    "for family, cluster_list in cluster_list_per_family.items():\n",
    "    print(f\"\\n→ Aligning {len(cluster_list)} clusters for family {family!r}\")\n",
    "    cluster_alignments = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments\"\n",
    "\n",
    "    # # clean alignments with stopcodons in the middle of the sequence (MACSE output \"!\" with frameshift/stop codons. Replace by \"NNN\" for analysis:)\n",
    "    for cluster in cluster_list:\n",
    "         cmd = (\n",
    "             f\"macse -prog exportAlignment \"\n",
    "             f\"-align {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments/{cluster}_NT.fa \"\n",
    "             f\"-codonForInternalStop NNN \"\n",
    "             f\"-codonForInternalFS --- \"\n",
    "             f\"-charForRemainingFS --- \"\n",
    "             f\"-out_NT {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments/{cluster}_NT.fa \"\n",
    "             f\"-out_AA {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments/{cluster}_AA.fa\"\n",
    "         )\n",
    "         subprocess.run(cmd, shell=True, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) # run quietly \n",
    "    \n",
    "    print(f\"→ Done family {family!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea14aa5-dd33-4a8d-af0d-7c5988f0fd1b",
   "metadata": {},
   "source": [
    "### Calculate dN & dS and N & S counts for each pairwise comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d677f-72fb-4fd0-bbd2-164cdd402c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate SYNonymous substitutions rate\n",
    "# Modified Nei-Gojobori with complete deletion\n",
    "for family in families:\n",
    "    # 1) ensure the output directory exists\n",
    "    ds_dir = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_dS\"\n",
    "    os.makedirs(ds_dir, exist_ok=True)\n",
    "\n",
    "    # 2) get the (previously filtered) list of clusters for this family\n",
    "    clusters = cluster_list_per_family[family]\n",
    "\n",
    "    # 3) run your original megacc command for each cluster\n",
    "    for cluster in clusters:\n",
    "        cmd = (\n",
    "            f\"megacc \"\n",
    "            f\"-a {data_dir}/compute_ds_modNG_compldel.mao \"\n",
    "            #f\"-d {data_dir}/sequences_x/{family}_isoform_X1/blastdb/cluster_alignments/{cluster}.meg \"\n",
    "            f\"-d {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments/{cluster}_NT.fa \"\n",
    "            f\"-o {ds_dir}/{cluster}\"\n",
    "        )\n",
    "        subprocess.run(cmd,shell=True, check=True,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8b4fd-822c-4e3c-9889-6506c23ced93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate NONsynonymous substitutions rate\n",
    "# Modified Nei-Gojobori with complete deletion\n",
    "for family in families:\n",
    "    # 1) ensure the output directory exists\n",
    "    ds_dir = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_dN\"\n",
    "    os.makedirs(ds_dir, exist_ok=True)\n",
    "\n",
    "    # 2) get the (previously filtered) list of clusters for this family\n",
    "    clusters = cluster_list_per_family[family]\n",
    "\n",
    "    # 3) run your original megacc command for each cluster\n",
    "    for cluster in clusters:\n",
    "        cmd = (\n",
    "            f\"megacc \"\n",
    "            f\"-a {data_dir}/compute_dN_modNG_compldel.mao \"\n",
    "            #f\"-d {data_dir}/sequences/{family}_isoform_X1/blastdb/cluster_alignments/{cluster}.meg \"\n",
    "            f\"-d {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments/{cluster}_NT.fa \"\n",
    "            f\"-o {ds_dir}/{cluster}\"\n",
    "        )\n",
    "        subprocess.run(cmd,shell=True, check=True,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ad309-64fc-4451-bc47-f2c58666c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate SYNonymous substitution COUNTS\n",
    "#actual counts of synonymous differences\n",
    "for family in families:\n",
    "    # 1) ensure the output directory exists\n",
    "    ds_dir = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_syn_count\"\n",
    "    os.makedirs(ds_dir, exist_ok=True)\n",
    "\n",
    "    # 2) get the (previously filtered) list of clusters for this family\n",
    "    clusters = cluster_list_per_family[family]\n",
    "\n",
    "    # 3) run your original megacc command for each cluster\n",
    "    for cluster in clusters:\n",
    "        cmd = (\n",
    "            f\"megacc \"\n",
    "            f\"-a {data_dir}/compute_syn_count_modNG_compldel.mao \"\n",
    "            #f\"-d {data_dir}/sequences/{family}_isoform_X1/blastdb/cluster_alignments/{cluster}.meg \"\n",
    "            f\"-d {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments/{cluster}_NT.fa \"\n",
    "            f\"-o {ds_dir}/{cluster}\"\n",
    "        )\n",
    "        subprocess.run(cmd,shell=True, check=True,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783253c8-5454-490f-a331-e633e095cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate NONSYNonymous substitution COUNTS\n",
    "#actual counts of nonsynonymous differences\n",
    "for family in families:\n",
    "    # 1) ensure the output directory exists\n",
    "    ds_dir = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_nonsyn_count\"\n",
    "    os.makedirs(ds_dir, exist_ok=True)\n",
    "\n",
    "    # 2) get the (previously filtered) list of clusters for this family\n",
    "    clusters = cluster_list_per_family[family]\n",
    "\n",
    "    # 3) run your original megacc command for each cluster\n",
    "    for cluster in clusters:\n",
    "        cmd = (\n",
    "            f\"megacc \"\n",
    "            f\"-a {data_dir}/compute_nonsyn_count_modNG_compldel.mao \"\n",
    "            #f\"-d {data_dir}/sequences/{family}_isoform_X1/blastdb/cluster_alignments/{cluster}.meg \"\n",
    "            f\"-d {data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/cluster_alignments/{cluster}_NT.fa \"\n",
    "            f\"-o {ds_dir}/{cluster}\"\n",
    "        )\n",
    "        subprocess.run(cmd,shell=True, check=True,stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ea256-5455-4ed9-a7f0-d8d13bd32698",
   "metadata": {},
   "source": [
    "### Turn .meg files into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73b078-be73-4c66-aaea-1df19c2911e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing parser (define once)\n",
    "def parse_meg_file(path: Path) -> pd.DataFrame:\n",
    "    text = path.read_text().splitlines()\n",
    "    labels = []\n",
    "    for line in text:\n",
    "        s = line.strip()\n",
    "        if re.match(r\"^\\[\\s*\\d\", s) and \"#\" not in s:\n",
    "            break\n",
    "        m = re.match(r\"^\\[\\s*(\\d+)\\]\\s*#\\s*(.+)$\", s)\n",
    "        if m:\n",
    "            labels.append(m.group(2).strip())\n",
    "    n = len(labels)\n",
    "    full = np.zeros((n, n), float)\n",
    "    for line in text:\n",
    "        s = line.strip()\n",
    "        m = re.match(r\"^\\[\\s*(\\d+)\\]\\s*(.*)$\", s)\n",
    "        if not m: continue\n",
    "        i = int(m.group(1))\n",
    "        if not (1 <= i <= n): continue\n",
    "        rest = m.group(2)\n",
    "        nums = re.findall(r\"[-+]?\\d*\\.\\d+(?:[eE][-+]?\\d+)?\", rest)\n",
    "        if len(nums) != i - 1: continue\n",
    "        for k, tok in enumerate(nums):\n",
    "            j = k + 1\n",
    "            v = float(tok)\n",
    "            full[i-1, j-1] = full[j-1, i-1] = v\n",
    "    np.fill_diagonal(full, 0.0)\n",
    "    df = pd.DataFrame(full, index=labels, columns=labels)\n",
    "    mask = np.tril(np.ones(df.shape, bool), k=-1)\n",
    "    return df.where(mask)\n",
    "\n",
    "# base data directory\n",
    "data_dir = Path(\"/home/emma/Amplicons/Workspaces/emma/downloaded_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74c6ac-61dd-4ad3-8350-42ef638c8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the per‐family loop SYNONYMOUS RATE\n",
    "for family in families:\n",
    "    print(f\"\\n→ Processing dS matrices for family {family!r}\")\n",
    "    \n",
    "    in_dir  = data_dir / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_dS\"\n",
    "    out_dir = in_dir / \"matrix_csvs\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for meg in sorted(in_dir.glob(\"*.meg\")):\n",
    "        df_lower = parse_meg_file(meg)\n",
    "        out_file = out_dir / f\"{meg.stem}_dS.csv\"\n",
    "        df_lower.to_csv(out_file)\n",
    "        #print(f\"   • wrote {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f43800-20ef-42e7-8d38-78b997b96316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the per‐family loop NONSYNONYMOUS RATE\n",
    "for family in families:\n",
    "    print(f\"\\n→ Processing dN matrices for family {family!r}\")\n",
    "    \n",
    "    in_dir  = data_dir / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_dN\"\n",
    "    out_dir = in_dir / \"matrix_csvs\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for meg in sorted(in_dir.glob(\"*.meg\")):\n",
    "        df_lower = parse_meg_file(meg)\n",
    "        out_file = out_dir / f\"{meg.stem}_dN.csv\"\n",
    "        df_lower.to_csv(out_file)\n",
    "        #print(f\"   • wrote {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2c0e3-86d5-4fc4-a73c-ebc312418932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the per‐family loop SYNONYMOUS RATE\n",
    "for family in families:\n",
    "    print(f\"\\n→ Processing dS matrices for family {family!r}\")\n",
    "    \n",
    "    in_dir  = data_dir / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_syn_count\"\n",
    "    out_dir = in_dir / \"matrix_csvs\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for meg in sorted(in_dir.glob(\"*.meg\")):\n",
    "        df_lower = parse_meg_file(meg)\n",
    "        out_file = out_dir / f\"{meg.stem}_S_count.csv\"\n",
    "        df_lower.to_csv(out_file)\n",
    "        #print(f\"   • wrote {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e89da-aa2b-45a7-ba68-22e5bf3227e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, the per‐family loop NONSYNONYMOUS RATE\n",
    "for family in families:\n",
    "    print(f\"\\n→ Processing dS matrices for family {family!r}\")\n",
    "    \n",
    "    in_dir  = data_dir / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_nonsyn_count\"\n",
    "    out_dir = in_dir / \"matrix_csvs\"\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for meg in sorted(in_dir.glob(\"*.meg\")):\n",
    "        df_lower = parse_meg_file(meg)\n",
    "        out_file = out_dir / f\"{meg.stem}_N_count.csv\"\n",
    "        df_lower.to_csv(out_file)\n",
    "        #print(f\"   • wrote {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97d9df-2a31-4cee-a008-7d208fe4a2b8",
   "metadata": {},
   "source": [
    "### Combine all the synonymous and nonsynonymous tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3e2b0-fc61-421e-aae5-214811615124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to extract the number of codons -> this will be called afterwards om the big table below\n",
    "def get_num_sites(cluster):\n",
    "    \"\"\"\n",
    "    Try to pull the reported “No. of Sites=” from the .meg file.\n",
    "    If that fails, parse the alignment itself and return length/3.\n",
    "    \"\"\"\n",
    "    meg_path = (\n",
    "        data_dir\n",
    "        / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_dN\"\n",
    "        / f\"{family}_cluster_{cluster}.meg\"\n",
    "    )\n",
    "    text = meg_path.read_text()\n",
    "    # 1) look for “No. of Sites = 123” with any spacing\n",
    "    m = re.search(r\"No\\.?\\s*of\\s*Sites\\s*=\\s*(\\d+)\", text, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    # 2) fallback: parse the alignment and count columns\n",
    "    try:\n",
    "        aln = AlignIO.read(str(meg_path), \"mega\")\n",
    "        # alignment.get_alignment_length() gives total columns;\n",
    "        # since this is codon‐alignment file, divide by 3\n",
    "        return aln.get_alignment_length() // 3\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b3582-8f83-4888-bd43-297af1bb7ece",
   "metadata": {},
   "source": [
    "### Between species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d8a1b-f9b3-4950-8fb3-d6b5ebb24b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# — assume you already have:\n",
    "#    families    : list of family names, e.g. ['NXF','ABC',…]\n",
    "#    data_dir    : Path to \"/home/emma/Amplicons/Workspaces/emma/downloaded_data\"\n",
    "#    species_list: list of species codes\n",
    "\n",
    "for family in families:\n",
    "    print(f\"\\n→ Processing between‐species stats for family {family}\")\n",
    "\n",
    "    # 1) define syn/nonsyn CSV directories\n",
    "    syn_dir    = data_dir / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_dS/matrix_csvs\"\n",
    "    nonsyn_dir = data_dir / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_dN/matrix_csvs\"\n",
    "\n",
    "    # 2) discover cluster IDs\n",
    "    clusters = []\n",
    "    for f in syn_dir.glob(f\"{family}_cluster_*_dS.csv\"):\n",
    "        m = re.match(rf\"{family}_cluster_(.+)_dS\\.csv\", f.name)\n",
    "        if m:\n",
    "            clusters.append(m.group(1))\n",
    "    clusters = sorted(set(clusters))\n",
    "    print(\"  Found clusters:\", clusters)\n",
    "\n",
    "    # helper: compute stats for a species pair in a lower‐triangle df\n",
    "    def pair_stats(df, a, b):\n",
    "        r = df.index.to_series().str.contains\n",
    "        c = df.columns.to_series().str.contains\n",
    "        mask = np.outer(r(a), c(b)) | np.outer(r(b), c(a))\n",
    "        vals = df.where(mask).stack()\n",
    "        return vals.mean(), vals.std(ddof=1)\n",
    "\n",
    "    # 3) load syn/nonsyn matrices and compute mean/SD\n",
    "    records = []\n",
    "    for cluster in clusters:\n",
    "        syn_df    = (pd.read_csv(syn_dir/f\"{family}_cluster_{cluster}_dS.csv\", index_col=0)\n",
    "                       .sort_index().sort_index(axis=1))\n",
    "        nonsyn_df = (pd.read_csv(nonsyn_dir/f\"{family}_cluster_{cluster}_dN.csv\", index_col=0)\n",
    "                       .sort_index().sort_index(axis=1))\n",
    "        for sp1, sp2 in itertools.combinations(species_list, 2):\n",
    "            m_s, sd_s = pair_stats(syn_df,    sp1, sp2)\n",
    "            m_n, sd_n = pair_stats(nonsyn_df, sp1, sp2)\n",
    "            records.append({\n",
    "                \"Cluster\":      cluster,\n",
    "                \"Species1\":     sp1,\n",
    "                \"Species2\":     sp2,\n",
    "                \"Mean_Syn\":     m_s,\n",
    "                \"SD_Syn\":       sd_s,\n",
    "                \"Mean_Nonsyn\":  m_n,\n",
    "                \"SD_Nonsyn\":    sd_n\n",
    "            })\n",
    "\n",
    "    master = pd.DataFrame(records)\n",
    "    cols = [\"Mean_Syn\",\"SD_Syn\",\"Mean_Nonsyn\",\"SD_Nonsyn\"]\n",
    "    master_clean = master.dropna(subset=cols, how=\"all\")\n",
    "\n",
    "    # 4) pull “No. of Sites” (codons) from each cluster's .meg\n",
    "    site_map = {cl: get_num_sites(cl) for cl in master_clean[\"Cluster\"].unique()}\n",
    "    master_clean[\"No_of_Codon\"] = master_clean[\"Cluster\"].map(site_map)\n",
    "\n",
    "    # 5) compute dN/dS ratio\n",
    "    master_clean[\"dNdS\"] = master_clean[\"Mean_Nonsyn\"] / master_clean[\"Mean_Syn\"]\n",
    "\n",
    "    # 6) annotate copy‐numbers\n",
    "    counts_csv = (\n",
    "        data_dir\n",
    "        / f\"sequences_x_updated/{family}_selected_isoform/blastdb/species_cluster_counts_{family}.csv\"\n",
    "    )\n",
    "    counts_df = pd.read_csv(counts_csv).set_index(\"species\")\n",
    "    master_clean[\"Species1_num_copies\"] = [\n",
    "        counts_df.at[s, c]\n",
    "        for s, c in zip(master_clean[\"Species1\"], master_clean[\"Cluster\"])\n",
    "    ]\n",
    "    master_clean[\"Species2_num_copies\"] = [\n",
    "        counts_df.at[s, c]\n",
    "        for s, c in zip(master_clean[\"Species2\"], master_clean[\"Cluster\"])\n",
    "    ]\n",
    "\n",
    "    # 7) build S/N counts table and merge\n",
    "    rec2 = []\n",
    "    syn_cnt_dir    = data_dir / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_syn_count/matrix_csvs\"\n",
    "    nonsyn_cnt_dir = data_dir / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_nonsyn_count/matrix_csvs\"\n",
    "    for cluster in clusters:\n",
    "        s_df = (pd.read_csv(syn_cnt_dir/f\"{family}_cluster_{cluster}_S_count.csv\", index_col=0)\n",
    "                  .sort_index().sort_index(axis=1))\n",
    "        n_df = (pd.read_csv(nonsyn_cnt_dir/f\"{family}_cluster_{cluster}_N_count.csv\", index_col=0)\n",
    "                  .sort_index().sort_index(axis=1))\n",
    "        for sp1, sp2 in itertools.combinations(species_list, 2):\n",
    "            ms, ss = pair_stats(s_df, sp1, sp2)\n",
    "            mn, sn = pair_stats(n_df, sp1, sp2)\n",
    "            rec2.append({\n",
    "                \"Cluster\":           cluster,\n",
    "                \"Species1\":          sp1,\n",
    "                \"Species2\":          sp2,\n",
    "                \"Mean_Syn_count\":    ms,\n",
    "                \"SD_Syn_count\":      ss,\n",
    "                \"Mean_Nonsyn_count\": mn,\n",
    "                \"SD_Nonsyn_count\":   sn\n",
    "            })\n",
    "\n",
    "    counts_total = (\n",
    "        pd.DataFrame(rec2)\n",
    "        .dropna(subset=[\"Mean_Syn_count\",\"SD_Syn_count\",\"Mean_Nonsyn_count\",\"SD_Nonsyn_count\"], how=\"all\")\n",
    "    )\n",
    "\n",
    "    final = (\n",
    "        master_clean\n",
    "        .merge(counts_total, on=[\"Cluster\",\"Species1\",\"Species2\"], how=\"left\")\n",
    "        .round(4)\n",
    "    )\n",
    "    final[[\"Species1_num_copies\",\"Species2_num_copies\"]] = final[[\"Species1_num_copies\",\"Species2_num_copies\"]].astype(int)\n",
    "\n",
    "    # 8) compute “potential synonymous sites” and adjusted dN/dS\n",
    "    final[\"pot_syn_sites\"] = final[\"Mean_Syn_count\"] / final[\"Mean_Syn\"]\n",
    "    final[\"adj_dNdS\"]      = (final[\"Mean_Nonsyn\"]) / (\n",
    "        (final[\"Mean_Syn_count\"] + 1) / final[\"pot_syn_sites\"]\n",
    "    )\n",
    "    \n",
    "    # 9) save\n",
    "    out_tsv = (\n",
    "        data_dir\n",
    "        / f\"sequences_x_updated/{family}_selected_isoform/blastdb/{family}_dN_dS_betweenspecies.tsv\"\n",
    "    )\n",
    "    final.to_csv(out_tsv, sep=\"\\t\", index=False)\n",
    "    print(f\"  → saved {out_tsv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a11b15-6a34-46f2-bc63-ceec140adf12",
   "metadata": {},
   "source": [
    "### Within species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700e508-cfd5-4eb5-8250-66ef10c95891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── ASSUMED DEFINED upstream ─────────────────────────────────────────\n",
    "# families     = ['NXF','ABC','XYZ', …]\n",
    "# data_dir     = Path(\"/home/emma/Amplicons/Workspaces/emma/downloaded_data\")\n",
    "# species_list = [\"HomSap\",\"PanTro\",\"PanPan\",\"GorGor\",\"PonPyg\",\"PonAbe\",\"SymSyn\",\"MacFas\"]\n",
    "\n",
    "for family in families:\n",
    "    print(f\"\\n→ Within‐species summary for family {family!r}\")\n",
    "\n",
    "    # 1) Directories for dS and dN matrices\n",
    "    syn_dir    = data_dir / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_dS/matrix_csvs\"\n",
    "    nonsyn_dir = data_dir / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_dN/matrix_csvs\"\n",
    "\n",
    "    # 2) Discover cluster IDs from the dS filenames\n",
    "    clusters = sorted({\n",
    "        re.match(rf\"{family}_cluster_(.+)_dS\\.csv\", p.name).group(1)\n",
    "        for p in syn_dir.glob(f\"{family}_cluster_*_dS.csv\")\n",
    "        if re.match(rf\"{family}_cluster_(.+)_dS\\.csv\", p.name)\n",
    "    })\n",
    "    print(\"  clusters:\", clusters)\n",
    "\n",
    "    # Helper to get mean & SD for a species in a lower‐triangle matrix\n",
    "    def pair_stats(df, a, b):\n",
    "        idx0 = df.index.to_series().str.contains\n",
    "        idx1 = df.columns.to_series().str.contains\n",
    "        mask = np.outer(idx0(a), idx1(b)) | np.outer(idx0(b), idx1(a))\n",
    "        vals = df.where(mask).stack()\n",
    "        return vals.mean(), vals.std(ddof=1)\n",
    "\n",
    "    # 3) Build within‐species dS/dN rates table\n",
    "    rate_records = []\n",
    "    for cluster in clusters:\n",
    "        s_df = (pd.read_csv(syn_dir/f\"{family}_cluster_{cluster}_dS.csv\", index_col=0)\n",
    "                  .sort_index().sort_index(axis=1))\n",
    "        n_df = (pd.read_csv(nonsyn_dir/f\"{family}_cluster_{cluster}_dN.csv\", index_col=0)\n",
    "                  .sort_index().sort_index(axis=1))\n",
    "        for sp in species_list:\n",
    "            m_s, sd_s = pair_stats(s_df, sp, sp)\n",
    "            m_n, sd_n = pair_stats(n_df, sp, sp)\n",
    "            rate_records.append({\n",
    "                \"Cluster\":      cluster,\n",
    "                \"Species\":      sp,\n",
    "                \"Mean_Syn\":     m_s,\n",
    "                \"SD_Syn\":       sd_s,\n",
    "                \"Mean_Nonsyn\":  m_n,\n",
    "                \"SD_Nonsyn\":    sd_n\n",
    "            })\n",
    "    within_rates = pd.DataFrame(rate_records).dropna(\n",
    "        subset=[\"Mean_Syn\",\"SD_Syn\",\"Mean_Nonsyn\",\"SD_Nonsyn\"],\n",
    "        how=\"all\"\n",
    "    )\n",
    "\n",
    "    # 4) Extract “No. of Sites” (codons) from each cluster’s .meg\n",
    "    site_map = {c: get_num_sites(c) for c in within_rates[\"Cluster\"].unique()}\n",
    "    within_rates[\"No_of_Codon\"] = within_rates[\"Cluster\"].map(site_map)\n",
    "\n",
    "    # 5) Compute dN/dS ratio\n",
    "    within_rates[\"dNdS\"] = within_rates[\"Mean_Nonsyn\"] / within_rates[\"Mean_Syn\"]\n",
    "\n",
    "    # 6) Annotate copy‐number from species×cluster counts\n",
    "    cnt_csv = (data_dir\n",
    "               / f\"sequences_x_updated/{family}_selected_isoform/blastdb/\"\n",
    "               / f\"species_cluster_counts_{family}.csv\")\n",
    "    cnt_df = pd.read_csv(cnt_csv).set_index(\"species\")\n",
    "    within_rates[\"num_copies\"] = [\n",
    "        cnt_df.at[row.Species, row.Cluster]\n",
    "        for _, row in within_rates.iterrows()\n",
    "    ]\n",
    "\n",
    "    # 7) Build within‐species raw count table (S_count / N_count)\n",
    "    count_records = []\n",
    "    syn_cnt_dir    = data_dir / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_syn_count/matrix_csvs\"\n",
    "    nonsyn_cnt_dir = data_dir / f\"sequences_x_updated/{family}_selected_isoform/blastdb/cluster_nonsyn_count/matrix_csvs\"\n",
    "    for cluster in clusters:\n",
    "        sf = syn_cnt_dir   / f\"{family}_cluster_{cluster}_S_count.csv\"\n",
    "        nf = nonsyn_cnt_dir/ f\"{family}_cluster_{cluster}_N_count.csv\"\n",
    "        if not (sf.exists() and nf.exists()):\n",
    "            print(f\"  ⚠ skipping counts for {cluster}: missing file\")\n",
    "            continue\n",
    "        s_df = pd.read_csv(sf, index_col=0).sort_index().sort_index(axis=1)\n",
    "        n_df = pd.read_csv(nf, index_col=0).sort_index().sort_index(axis=1)\n",
    "        for sp in species_list:\n",
    "            ms, ss = pair_stats(s_df, sp, sp)\n",
    "            mn, sn = pair_stats(n_df, sp, sp)\n",
    "            count_records.append({\n",
    "                \"Cluster\":           cluster,\n",
    "                \"Species\":           sp,\n",
    "                \"Mean_Syn_count\":    ms,\n",
    "                \"SD_Syn_count\":      ss,\n",
    "                \"Mean_Nonsyn_count\": mn,\n",
    "                \"SD_Nonsyn_count\":   sn\n",
    "            })\n",
    "    within_counts = pd.DataFrame(count_records).dropna(\n",
    "        subset=[\"Mean_Syn_count\",\"SD_Syn_count\",\"Mean_Nonsyn_count\",\"SD_Nonsyn_count\"],\n",
    "        how=\"all\"\n",
    "    )\n",
    "\n",
    "    # 8) Merge rates + counts into one within‐species table\n",
    "    within_species = within_rates.merge(\n",
    "        within_counts,\n",
    "        on=[\"Cluster\",\"Species\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "     # 9) Round decimals to 4 places, integers for codons & copies\n",
    "    dec_cols = [\n",
    "        \"Mean_Syn\",\"SD_Syn\",\"Mean_Nonsyn\",\"SD_Nonsyn\",\n",
    "        \"Mean_Syn_count\",\"SD_Syn_count\",\n",
    "        \"Mean_Nonsyn_count\",\"SD_Nonsyn_count\",\"dNdS\"\n",
    "    ]\n",
    "    within_species[dec_cols] = within_species[dec_cols].round(4)\n",
    "    int_cols = [\"No_of_Codon\",\"num_copies\"]\n",
    "    within_species[int_cols] = within_species[int_cols].round(0).astype(int)\n",
    "\n",
    "    # 10) Save the combined table\n",
    "    out_file = (data_dir\n",
    "                / f\"sequences_x_updated/{family}_selected_isoform/blastdb/\"\n",
    "                / f\"{family}_dN_dS_withinspecies.tsv\")\n",
    "    within_species.to_csv(out_file, sep=\"\\t\", index=False)\n",
    "    print(f\"  • saved combined within‐species table → {out_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb91991-34cf-4074-8e3e-e5c52446aeec",
   "metadata": {},
   "source": [
    "### Combine all tables together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1a6a5-fbe1-4cf8-a4a2-d27725a3dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETWEEN SPECIES \n",
    "\n",
    "## Combine in one large table for all families together \n",
    "# ── ASSUMED DEFINED UPSTREAM ─────────────────────────────────────────\n",
    "# families = ['NXF','ABC','XYZ', …]\n",
    "# data_dir = Path(\"/home/emma/Amplicons/Workspaces/emma/downloaded_data\")\n",
    "\n",
    "# 1) Read each per-family TSV, add a \"Family\" column, collect into a list\n",
    "tables = []\n",
    "for family in families:\n",
    "    path = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/{family}_dN_dS_betweenspecies.tsv\"\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    df[\"Family\"] = family\n",
    "    tables.append(df)\n",
    "\n",
    "# 2) Concatenate them all into one DataFrame\n",
    "combined = pd.concat(tables, ignore_index=True)\n",
    "\n",
    "# 3) Save the big table\n",
    "out = f\"{data_dir}/sequences_x_updated/all_families_dN_dS_betweenspecies.tsv\"\n",
    "combined.to_csv(out, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"→ Wrote combined table with {len(combined)} rows to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbefcd-c0a1-4083-906f-1d02f630b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHIN SPECIES \n",
    "\n",
    "## Combine in one large table for all families together \n",
    "# ── ASSUMED DEFINED UPSTREAM ─────────────────────────────────────────\n",
    "# families = ['NXF','ABC','XYZ', …]\n",
    "# data_dir = Path(\"/home/emma/Amplicons/Workspaces/emma/downloaded_data\")\n",
    "\n",
    "# 1) Read each per-family TSV, add a \"Family\" column, collect into a list\n",
    "tables = []\n",
    "for family in families:\n",
    "    path = f\"{data_dir}/sequences_x_updated/{family}_selected_isoform/blastdb/{family}_dN_dS_withinspecies.tsv\"\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    df[\"Family\"] = family\n",
    "    tables.append(df)\n",
    "\n",
    "# 2) Concatenate them all into one DataFrame\n",
    "combined = pd.concat(tables, ignore_index=True)\n",
    "\n",
    "# 3) Save the big table\n",
    "out = f\"{data_dir}/sequences_x_updated/all_families_dN_dS_withinspecies.tsv\"\n",
    "combined.to_csv(out, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"→ Wrote combined table with {len(combined)} rows to {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b71bb-e2f3-4c08-b9ff-d306434fb39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cluster counts overview\n",
    "# DataFrame with gene coordinates and other details (adjust filename if needed)\n",
    "genes_y = pd.read_csv(f\"{data_dir}/sequences_y_updated/all_families_gene_details_with_clusters.tsv\", sep='\\t')\n",
    "genes_x = pd.read_csv(f\"{data_dir}/sequences_x_updated/all_families_gene_details_with_clusters.tsv\", sep='\\t')\n",
    "genes_y_long = pd.read_csv(f\"{data_dir}/sequences_y_longestisoform/all_families_gene_details_with_clusters.tsv\", sep='\\t')\n",
    "genes_x_long = pd.read_csv(f\"{data_dir}/sequences_x_longestisoform/all_families_gene_details_with_clusters.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04454d9c-9949-44f2-8fcc-161b401dad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dataframe into Family, cluster, species counts\n",
    "# 1) Count how many genes per Family + Cluster + Species\n",
    "counts_y = (\n",
    "    genes_y\n",
    "      .groupby(['gene_family_symbol', 'cluster', 'Species'])\n",
    "      .size()                                # number of rows in each group\n",
    "      .unstack(fill_value=0)                # turn Species into columns\n",
    "      .reset_index()                        # bring family & cluster back as cols\n",
    ")\n",
    "\n",
    "# 2) Rename for clarity\n",
    "counts_y = counts_y.rename(\n",
    "    columns={\n",
    "      'gene_family_symbol': 'Family',\n",
    "      'cluster':            'Cluster'\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3) View result\n",
    "counts_y\n",
    "\n",
    "# save\n",
    "counts_y.to_csv(f\"{data_dir}/sequences_y_updated/cluster_counts_perspecies.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e1bd2-4e75-4686-bfcb-89bc9e649e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dataframe into Family, cluster, species counts\n",
    "# 1) Count how many genes per Family + Cluster + Species\n",
    "counts_x = (\n",
    "    genes_x\n",
    "      .groupby(['gene_family_symbol', 'cluster', 'Species'])\n",
    "      .size()                                # number of rows in each group\n",
    "      .unstack(fill_value=0)                # turn Species into columns\n",
    "      .reset_index()                        # bring family & cluster back as cols\n",
    ")\n",
    "\n",
    "# 2) Rename for clarity\n",
    "counts_x = counts_x.rename(\n",
    "    columns={\n",
    "      'gene_family_symbol': 'Family',\n",
    "      'cluster':            'Cluster'\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3) View result\n",
    "counts_x\n",
    "\n",
    "# save\n",
    "counts_x.to_csv(f\"{data_dir}/sequences_x_updated/cluster_counts_perspecies.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23210124-c3b5-4a5f-89d2-3b93d4dd804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dataframe into Family, cluster, species counts\n",
    "# 1) Count how many genes per Family + Cluster + Species\n",
    "counts_y_long = (\n",
    "    genes_y_long\n",
    "      .groupby(['gene_family_symbol', 'cluster', 'Species'])\n",
    "      .size()                                # number of rows in each group\n",
    "      .unstack(fill_value=0)                # turn Species into columns\n",
    "      .reset_index()                        # bring family & cluster back as cols\n",
    ")\n",
    "\n",
    "# 2) Rename for clarity\n",
    "counts_y_long = counts_y_long.rename(\n",
    "    columns={\n",
    "      'gene_family_symbol': 'Family',\n",
    "      'cluster':            'Cluster'\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3) View result\n",
    "counts_y_long\n",
    "\n",
    "# save\n",
    "counts_y_long.to_csv(f\"{data_dir}/sequences_y_longestisoform/cluster_counts_perspecies.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e74e3e-fade-4856-8cb8-a33978a2655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dataframe into Family, cluster, species counts\n",
    "# 1) Count how many genes per Family + Cluster + Species\n",
    "counts_x_long = (\n",
    "    genes_x_long\n",
    "      .groupby(['gene_family_symbol', 'cluster', 'Species'])\n",
    "      .size()                                # number of rows in each group\n",
    "      .unstack(fill_value=0)                # turn Species into columns\n",
    "      .reset_index()                        # bring family & cluster back as cols\n",
    ")\n",
    "\n",
    "# 2) Rename for clarity\n",
    "counts_x_long = counts_x_long.rename(\n",
    "    columns={\n",
    "      'gene_family_symbol': 'Family',\n",
    "      'cluster':            'Cluster'\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3) View result\n",
    "counts_x_long\n",
    "\n",
    "# save\n",
    "counts_x_long.to_csv(f\"{data_dir}/sequences_x_longestisoform/cluster_counts_perspecies.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e38d73-3bff-441f-9053-b22f79895935",
   "metadata": {},
   "source": [
    "## After bootstrapping merge dNdS analysis\n",
    "#### The bootstrapping is done by is another script !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b552645-ec24-4e87-a5f1-ca2861d3fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bootstrap information\n",
    "bootstrap_y = pd.read_csv(f'/home/emma/Amplicons/Workspaces/emma/downloaded_data/X_updated_bootstrap_results_20251101_140844/bootstrap_results.csv',sep=\",\")\n",
    "bootstrap_y\n",
    "\n",
    "bootstrap = bootstrap_y\n",
    "# add new columns that state the amount of times you have \"0\" in the bootstrapped list\n",
    "import numpy as np\n",
    "\n",
    "# Helper function: turn comma-separated string into list of floats, then compute fraction of zeros\n",
    "def frac_zeros(val):\n",
    "    # split by comma, convert to float\n",
    "    arr = np.array([float(x) for x in val.split(\",\")])\n",
    "    return np.mean(arr == 0)\n",
    "\n",
    "# Apply to each column\n",
    "bootstrap[\"dN_fraction_zeros\"] = bootstrap[\"dN_rates\"].apply(frac_zeros)\n",
    "bootstrap[\"dS_fraction_zeros\"] = bootstrap[\"dS_rates\"].apply(frac_zeros)\n",
    "\n",
    "# calculate mean dNdS + mean dS + mean dN\n",
    "# keep in mind paired bootrsapped. \n",
    "# calculate the mean of the ratios mean(dN[i]/dS[i])\n",
    "import numpy as np\n",
    "\n",
    "def calc_bootstrap_stats(dn_str, ds_str):\n",
    "    # Parse strings into float arrays\n",
    "    dn = np.array([float(x) for x in dn_str.split(\",\")])\n",
    "    ds = np.array([float(x) for x in ds_str.split(\",\")])\n",
    "    \n",
    "    # --- mean of ratios ---\n",
    "    ratios = np.divide(dn, ds, out=np.full_like(dn, np.nan), where=ds!=0)\n",
    "    mean_dnds = np.nanmean(ratios)\n",
    "    \n",
    "    # --- mean dN and mean dS ---\n",
    "    mean_dn = np.mean(dn)\n",
    "    mean_ds = np.mean(ds)\n",
    "    \n",
    "    return mean_dnds, mean_dn, mean_ds\n",
    "\n",
    "\n",
    "# Apply row-wise\n",
    "bootstrap[[\"mean_dNdS\", \"mean_dN\", \"mean_dS\"]] = bootstrap.apply(\n",
    "    lambda row: pd.Series(calc_bootstrap_stats(row[\"dN_rates\"], row[\"dS_rates\"])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# counts but without filtering out the ones with division of dS  by zero -> this would just give infinite which would count as above 1 ? \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def count_ratio_below_above_1(dn_str, ds_str):\n",
    "    # Parse comma-separated strings into float arrays (fast & robust)\n",
    "    dn = np.fromstring(dn_str, sep=\",\")\n",
    "    ds = np.fromstring(ds_str, sep=\",\")\n",
    "\n",
    "    # dN/dS with dS==0 -> +inf (including 0/0)\n",
    "    ratios = np.divide(dn, ds, out=np.full_like(dn, np.inf), where=ds != 0)\n",
    "\n",
    "    # Counts (NaNs won’t occur with the logic above; inf > 1 evaluates True)\n",
    "    below = int(np.sum(ratios < 1))\n",
    "    above = int(np.sum(ratios > 1))  # counts +inf as above 1 automatically\n",
    "\n",
    "    return below, above\n",
    "\n",
    "# Apply row-wise\n",
    "bootstrap[[\"dNdS_count_below1\", \"dNdS_count_above1\"]] = bootstrap.apply(\n",
    "    lambda row: pd.Series(count_ratio_below_above_1(row[\"dN_rates\"], row[\"dS_rates\"])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# bootstrap\n",
    "ALPHA   = 0.05\n",
    "BOOT_N  = 10000  # total bootstraps\n",
    "\n",
    "# Fractions\n",
    "bootstrap[\"frac_below1\"] = bootstrap[\"dNdS_count_below1\"] / BOOT_N\n",
    "bootstrap[\"frac_above1\"] = bootstrap[\"dNdS_count_above1\"] / BOOT_N\n",
    "\n",
    "# (optional) if you want to track the mass exactly at 1 (and any NaNs if present)\n",
    "# This assumes no double-counting across the *_count_* columns.\n",
    "if \"dNdS_count_nan\" in bootstrap.columns:\n",
    "    bootstrap[\"frac_equal1\"] = (BOOT_N - bootstrap[\"dNdS_count_below1\"]\n",
    "                                          - bootstrap[\"dNdS_count_above1\"]\n",
    "                                          - bootstrap[\"dNdS_count_nan\"]) / BOOT_N\n",
    "else:\n",
    "    bootstrap[\"frac_equal1\"] = (BOOT_N - bootstrap[\"dNdS_count_below1\"]\n",
    "                                          - bootstrap[\"dNdS_count_above1\"]) / BOOT_N\n",
    "\n",
    "# Safety: clip numerical slop\n",
    "bootstrap[[\"frac_below1\",\"frac_above1\",\"frac_equal1\"]] = \\\n",
    "    bootstrap[[\"frac_below1\",\"frac_above1\",\"frac_equal1\"]].clip(lower=0, upper=1)\n",
    "\n",
    "# Labels (use ≤ per your definition). Treat the rare case where both sides ≤ α\n",
    "# (i.e., most mass is exactly 1) as \"neutral (~1)\".\n",
    "positive   = bootstrap[\"frac_below1\"] <= ALPHA\n",
    "purifying  = bootstrap[\"frac_above1\"] <= ALPHA\n",
    "neutralish = positive & purifying      # e.g., ~all mass at exactly 1\n",
    "\n",
    "bootstrap[\"selection\"] = np.select(\n",
    "    [neutralish,            positive,     purifying],\n",
    "    [\"neutral (~1)\",        \"positive\",   \"purifying\"],\n",
    "    default=\"nonsignificant\"\n",
    ")\n",
    "\n",
    "# (optional) p-values for one-sided bootstrap tests\n",
    "#bootstrap[\"p_pos\"] = bootstrap[\"frac_below1\"]  # P(dN/dS ≤ 1)\n",
    "#bootstrap[\"p_pur\"] = bootstrap[\"frac_above1\"]  # P(dN/dS ≥ 1)\n",
    "\n",
    "# tidy display\n",
    "cols_to_round = [\"frac_below1\",\"frac_above1\",\"frac_equal1\"]\n",
    "bootstrap[cols_to_round] = bootstrap[cols_to_round].round(4)\n",
    "bootstrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acac3c8-11be-46a0-98af-9adbd35c5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dNdS pairwise dataframe \n",
    "y_between_overview = pd.read_csv(f'/home/emma/Amplicons/Workspaces/emma/downloaded_data/sequences_x_updated/all_families_dN_dS_betweenspecies.tsv',sep=\"\\t\")\n",
    "y_within_overview = pd.read_csv(f'/home/emma/Amplicons/Workspaces/emma/downloaded_data/sequences_x_updated/all_families_dN_dS_withinspecies.tsv',sep=\"\\t\")\n",
    "# merge the 2 dataframes\n",
    "# --- Step 1: Modify x_within_overview ---\n",
    "y_within_modified = y_within_overview.copy()\n",
    "\n",
    "# Duplicate num_copies -> Species1_num_copies and Species2_num_copies\n",
    "y_within_modified[\"Species1_num_copies\"] = y_within_modified[\"num_copies\"]\n",
    "y_within_modified[\"Species2_num_copies\"] = y_within_modified[\"num_copies\"]\n",
    "\n",
    "# Duplicate Species -> Species1 and Species2\n",
    "y_within_modified[\"Species1\"] = y_within_modified[\"Species\"]\n",
    "y_within_modified[\"Species2\"] = y_within_modified[\"Species\"]\n",
    "\n",
    "# Drop the old single-species columns\n",
    "y_within_modified = y_within_modified.drop(columns=[\"num_copies\", \"Species\"])\n",
    "\n",
    "# --- Step 2: Modify x_between_overview ---\n",
    "y_between_modified = y_between_overview.drop(columns=[\"pot_syn_sites\", \"adj_dNdS\"])\n",
    "\n",
    "# --- Step 3: Align and merge ---\n",
    "# Ensure same column order\n",
    "y_within_modified = y_within_modified[y_between_modified.columns]\n",
    "\n",
    "# Concatenate\n",
    "merged_overview_y = pd.concat([y_between_modified, y_within_modified], ignore_index=True)\n",
    "merged_overview_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba1be3-96c5-484e-a06a-87a37835d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left horizontal merge of merged_overview and bootstrap dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Keep only the columns you need from bootstrap\n",
    "boot_cols = [\n",
    "    \"family\", \"cluster\", \"species1\", \"species2\",\n",
    "    \"mean_dNdS\", \"mean_dN\", \"mean_dS\",\n",
    "    \"frac_below1\", \"frac_above1\", \"selection\"\n",
    "]\n",
    "boot_sub = bootstrap[boot_cols].copy()\n",
    "\n",
    "# 2) Rename bootstrap key columns to match merged_overview\n",
    "boot_sub = boot_sub.rename(columns={\n",
    "    \"family\": \"Family\",\n",
    "    \"cluster\": \"Cluster\",\n",
    "    \"species1\": \"Species1\",\n",
    "    \"species2\": \"Species2\"\n",
    "})\n",
    "\n",
    "# 2) Clean Cluster: keep only text after \"_cluster_\"\n",
    "boot_sub[\"Cluster\"] = (\n",
    "    boot_sub[\"Cluster\"].astype(str)\n",
    "    .str.split(pat=\"_cluster_\", n=1, expand=False)\n",
    "    .str[-1]\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# --- starting from your prepared `boot_sub` (with Cluster cleaned) and `merged_overview` ---\n",
    "\n",
    "def with_canonical_species(df):\n",
    "    out = df.copy()\n",
    "    s1 = out[\"Species1\"].astype(str).str.strip()\n",
    "    s2 = out[\"Species2\"].astype(str).str.strip()\n",
    "    # sort the pair case-insensitively so A–B == B–A\n",
    "    order = s1.str.lower() <= s2.str.lower()\n",
    "    out[\"_SpeciesA\"] = s1.where(order, s2)\n",
    "    out[\"_SpeciesB\"] = s2.where(order, s1)\n",
    "    return out\n",
    "\n",
    "mo = with_canonical_species(merged_overview_y)\n",
    "bs = with_canonical_species(boot_sub)\n",
    "\n",
    "# (optional) if bootstrap can have duplicates per canonical key, dedupe:\n",
    "bs = bs.drop_duplicates(subset=[\"Family\", \"Cluster\", \"_SpeciesA\", \"_SpeciesB\"])\n",
    "\n",
    "# Merge on Family, Cluster, and canonical species\n",
    "final_df_y = pd.merge(\n",
    "    mo,\n",
    "    bs[[\n",
    "        \"Family\", \"Cluster\", \"_SpeciesA\", \"_SpeciesB\",\n",
    "        \"mean_dNdS\", \"mean_dN\", \"mean_dS\", \"frac_below1\", \"frac_above1\", \"selection\"\n",
    "    ]],\n",
    "    how=\"left\",\n",
    "    on=[\"Family\", \"Cluster\", \"_SpeciesA\", \"_SpeciesB\"]\n",
    ")\n",
    "\n",
    "# Drop helper columns\n",
    "final_df_y = final_df_y.drop(columns=[\"_SpeciesA\", \"_SpeciesB\"])\n",
    "mask_both_zero = (final_df_y[\"mean_dN\"] == 0) & (final_df_y[\"mean_dS\"] == 0)\n",
    "final_df_y.loc[mask_both_zero, \"selection\"] = \"purifying\"\n",
    "final_df_y\n",
    "final_df_y.to_csv(f\"/home/emma/Amplicons/Workspaces/emma/downloaded_data/sequences_x_updated/Bootstrap_all_families_dN_dS_between_within_species_x_updated2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd9c0e-fad5-4075-baa6-7bddce4ae626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of the within species positive selection\n",
    "dnds_total = pd.read_csv(f\"{data_dir}/sequences_x_updated/Bootstrap_all_families_dN_dS_between_within_species_x_updated2.csv\")\n",
    "dnds_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e923f16-b421-4570-950e-5b41af226c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract within species \n",
    "# only positive selection filtering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Make a numeric helper column (strings like \"Inf\", \"inf\", \"Infinity\" become np.inf; bad text -> NaN)\n",
    "df = dnds_total.copy()\n",
    "df[\"dNdS_num\"] = pd.to_numeric(df[\"dNdS\"], errors=\"coerce\")\n",
    "\n",
    "# 2) Keep only finite values (this removes +inf/-inf and NaN)\n",
    "df = df[np.isfinite(df[\"dNdS_num\"])]\n",
    "\n",
    "# 3) Apply your filters and select columns\n",
    "cols = [\"Family\", \"Cluster\", \"Species1\", \"Species1_num_copies\", \"dNdS\", \"selection\"]\n",
    "dnds_filtered = df.loc[\n",
    "    (df[\"Species1\"] == df[\"Species2\"]) & (df[\"dNdS_num\"] > 1),\n",
    "    cols\n",
    "]\n",
    "\n",
    "#save\n",
    "dnds_filtered.to_csv(f\"/home/emma/Amplicons/Workspaces/emma/downloaded_data/sequences_x_updated/within_positive_selection.csv\", index=False)\n",
    "\n",
    "dnds_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fd9a9-7697-4f0e-b02e-3d0d7ae4680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by family-cluster\n",
    "df = dnds_filtered.copy()\n",
    "df[\"dNdS\"] = df[\"dNdS\"].astype(float).round(2)\n",
    "\n",
    "result = (\n",
    "    df.groupby([\"Family\", \"Cluster\"])\n",
    "      .agg({\n",
    "          \"Species1\": lambda x: list(x.unique()),\n",
    "          \"dNdS\": list,\n",
    "           \"Species1_num_copies\": list\n",
    "      })\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b957b1-79a2-4bf3-8755-d534a6c41cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
